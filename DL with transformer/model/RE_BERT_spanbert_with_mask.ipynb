{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20525,"status":"ok","timestamp":1709823653125,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"Y5yej-1ovufz","outputId":"122dd5f9-af6b-4b98-eea5-fb69a424f7e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/Drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/Drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14170,"status":"ok","timestamp":1709823667291,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"nanNQZCEMHDO","outputId":"622e57b3-7abf-468c-d083-06f0dd58c869"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"]}],"source":["!pip install torch torchvision torchaudio\n","!pip install transformers\n","!pip install tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4NHSnwIXv0na"},"outputs":[],"source":["import json\n","import random\n","import warnings\n","import torch\n","import time\n","import argparse\n","import json\n","import os\n","import time\n","import copy\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.autograd as autograd\n","import torch.nn.functional\n","\n","\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import MinMaxScaler\n","from transformers import BertForSequenceClassification, BertTokenizer, BertModel, AdamW"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709823673797,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"yeH_1AYRDb8U","outputId":"5c7b69ec-1d53-46ed-fb7c-18216d3d009c"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'org:country_of_headquarters': 3, 'per:parents': 4, 'per:age': 5, 'per:countries_of_residence': 6, 'per:children': 7, 'org:alternate_names': 8, 'per:charges': 9, 'per:cities_of_residence': 10, 'per:origin': 11, 'org:founded_by': 12, 'per:employee_of': 13, 'per:siblings': 14, 'per:alternate_names': 15, 'org:website': 16, 'per:religion': 17, 'per:stateorprovince_of_death': 18, 'org:parents': 19, 'org:subsidiaries': 20, 'per:other_family': 21, 'per:stateorprovinces_of_residence': 22, 'org:members': 23, 'per:cause_of_death': 24, 'org:member_of': 25, 'org:number_of_employees/members': 26, 'per:country_of_birth': 27, 'org:shareholders': 28, 'org:stateorprovince_of_headquarters': 29, 'per:city_of_death': 30, 'per:date_of_birth': 31, 'per:spouse': 32, 'org:city_of_headquarters': 33, 'per:date_of_death': 34, 'per:schools_attended': 35, 'org:political/religious_affiliation': 36, 'per:country_of_death': 37, 'org:founded': 38, 'per:stateorprovince_of_birth': 39, 'per:city_of_birth': 40, 'org:dissolved': 41}\n","{0: 'no_relation', 1: 'per:title', 2: 'org:top_members/employees', 3: 'org:country_of_headquarters', 4: 'per:parents', 5: 'per:age', 6: 'per:countries_of_residence', 7: 'per:children', 8: 'org:alternate_names', 9: 'per:charges', 10: 'per:cities_of_residence', 11: 'per:origin', 12: 'org:founded_by', 13: 'per:employee_of', 14: 'per:siblings', 15: 'per:alternate_names', 16: 'org:website', 17: 'per:religion', 18: 'per:stateorprovince_of_death', 19: 'org:parents', 20: 'org:subsidiaries', 21: 'per:other_family', 22: 'per:stateorprovinces_of_residence', 23: 'org:members', 24: 'per:cause_of_death', 25: 'org:member_of', 26: 'org:number_of_employees/members', 27: 'per:country_of_birth', 28: 'org:shareholders', 29: 'org:stateorprovince_of_headquarters', 30: 'per:city_of_death', 31: 'per:date_of_birth', 32: 'per:spouse', 33: 'org:city_of_headquarters', 34: 'per:date_of_death', 35: 'per:schools_attended', 36: 'org:political/religious_affiliation', 37: 'per:country_of_death', 38: 'org:founded', 39: 'per:stateorprovince_of_birth', 40: 'per:city_of_birth', 41: 'org:dissolved'}\n"]}],"source":["# List of relation types\n","keys = ['no_relation', 'per:title', 'org:top_members/employees',\n","        'org:country_of_headquarters', 'per:parents', 'per:age',\n","        'per:countries_of_residence', 'per:children', 'org:alternate_names',\n","        'per:charges', 'per:cities_of_residence', 'per:origin', 'org:founded_by',\n","        'per:employee_of', 'per:siblings', 'per:alternate_names', 'org:website',\n","        'per:religion', 'per:stateorprovince_of_death', 'org:parents',\n","        'org:subsidiaries', 'per:other_family', 'per:stateorprovinces_of_residence',\n","        'org:members', 'per:cause_of_death', 'org:member_of',\n","        'org:number_of_employees/members', 'per:country_of_birth',\n","        'org:shareholders', 'org:stateorprovince_of_headquarters',\n","        'per:city_of_death', 'per:date_of_birth', 'per:spouse',\n","        'org:city_of_headquarters', 'per:date_of_death', 'per:schools_attended',\n","        'org:political/religious_affiliation', 'per:country_of_death',\n","        'org:founded', 'per:stateorprovince_of_birth', 'per:city_of_birth',\n","        'org:dissolved']\n","\n","# Assigning indices to the list elements and storing them in a dictionary\n","rel2id = {key: idx for idx, key in enumerate(keys)}\n","id2rel = {idx: key for key, idx in rel2id.items()}\n","\n","# Printing the dictionaries\n","print(rel2id)\n","print(id2rel)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1709823673797,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"Welvz8AbMHDR","outputId":"e67bf5c6-bf9d-4d23-eebc-31936fb94d89"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n","using GPU\n"]}],"source":["# tell pytorch to use the gpu if available\n","if torch.cuda.is_available():\n","\n","    DEVICE = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    DEVICE = torch.device(\"cpu\")\n","\n","USE_CUDA = torch.cuda.is_available()\n","if USE_CUDA:\n","    print(\"using GPU\")\n","else:\n","    print(\"using CPU\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3488,"status":"ok","timestamp":1709823677275,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"R5bbj5oeMHDS","outputId":"2a66e59c-0830-49c6-ac34-b21323a4cc04"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT tokenizer loaded\n"]}],"source":["#BERT tokenizer\n","model_directory = '/content/Drive/MyDrive/COMP61332_text_mining/RE/Tacred/Bert_RE/span-bert/spanbert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(model_directory)\n","print('BERT tokenizer loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6No8Wu6SN95j"},"outputs":[],"source":["def setup_seed(seed):\n","    # Sets the seed for generating random numbers for the CPU.\n","\n","    # Sets the seed for generating random numbers for all GPUs.\n","    torch.cuda.manual_seed_all(seed)\n","\n","    # Sets the seed for generating random numbers with NumPy.\n","    np.random.seed(seed)\n","\n","    # Sets the seed for the built-in Python random module.\n","    random.seed(seed)\n","\n","setup_seed(44)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTmRuAmIv04k"},"outputs":[],"source":["def load_tacred_dataset(file_path):\n","    \"\"\"\n","    Load the TACRED dataset from a JSON file.\n","\n","    Args:\n","    file_path (str): The path to the JSON file containing the dataset.\n","\n","    Returns:\n","    dict: The loaded dataset.\n","    \"\"\"\n","    with open(file_path, 'r') as file:\n","        data = json.load(file)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vLsXtBwYv1EN"},"outputs":[],"source":["def prepare_data(data):\n","    # Get the number of instances in the data\n","    n = len(data)\n","\n","    # Initialize an empty list to store processed data\n","    info = []\n","\n","    # Iterate through each instance in the data\n","    for i in range(n):\n","        # Initialize an empty dictionary for storing processed information of a single data instance\n","        single_data = {}\n","\n","        # Extract start and end indices of subject and object entities\n","        ss = data[i][\"subj_start\"]\n","        se = data[i][\"subj_end\"]\n","        os = data[i][\"obj_start\"]\n","        oe = data[i][\"obj_end\"]\n","\n","        # Extract subject and object tokens based on their start and end indices\n","        subj = data[i]['token'][ss: se+1]\n","        obj = data[i]['token'][os: oe+1]\n","\n","        temp = data[i]['token'].copy()\n","\n","        temp[ss: se+1] = ['[MASK]']\n","        temp[os: oe+1] = ['[MASK]']\n","\n","        # Convert subject and object tokens into strings\n","        ent1 = ' '.join(subj)\n","        ent2 = ' '.join(obj)\n","\n","        # Extract relation label\n","        rel = data[i][\"relation\"]\n","\n","        # Concatenate all tokens to form the original sentence\n","        text = \" \".join(temp)\n","\n","        # Store processed information in the dictionary\n","        single_data['rel'] = rel\n","        single_data['ent1'] = ent1\n","        single_data['ent2'] = ent2\n","        single_data['text'] = text\n","\n","        # Append processed information of a single data instance to the list\n","        info.append(single_data)\n","\n","    # Return the processed data\n","    return info"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pD96gM1FDb--"},"outputs":[],"source":["def process_relation_extraction_data(info, max_length=64):\n","    # Initialize a dictionary to store processed data\n","    data = {}\n","    data['label'] = []  # List to store relation labels\n","    data['mask'] = []   # List to store attention masks\n","    data['text'] = []   # List to store tokenized and padded texts\n","\n","    # Iterate through each instance in the provided 'info' data\n","    for line in info:\n","        # Check if the relation label is present in the 'rel2id' dictionary\n","        if line['rel'] not in rel2id:\n","            # If the relation label is not found, assign label 0 (for unknown relation)\n","            data['label'].append(0)\n","        else:\n","            # If the relation label is found, assign its corresponding index\n","            data['label'].append(rel2id[line['rel']])\n","\n","        # Concatenate subject, object, and text to form a single sentence\n","        sent = line['ent1'] +'[SEP]'+line['ent2'] + '[SEP]' + line['text']\n","\n","        # Tokenize the concatenated sentence and add special tokens\n","        indexed_tokens = tokenizer.encode(sent, add_special_tokens=True)\n","\n","        # Determine the available length of the tokenized sentence\n","        avai_len = len(indexed_tokens)\n","\n","        # Pad the tokenized sentence with 0s to match the maximum length\n","        while len(indexed_tokens) < max_length:\n","            indexed_tokens.append(0)\n","\n","        # Trim the tokenized sentence to the maximum length\n","        indexed_tokens = indexed_tokens[:max_length]\n","\n","        # Convert the tokenized sentence to a PyTorch tensor\n","        indexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n","\n","        # Create an attention mask for the tokenized sentence\n","        att_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\n","        att_mask[0, :avai_len] = 1\n","\n","        # Append the tokenized and padded sentence, and its attention mask to the data dictionary\n","        data['text'].append(indexed_tokens)\n","        data['mask'].append(att_mask)\n","\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTsJ6q_dMHDW"},"outputs":[],"source":["def convert_data_to_tensors(data):\n","    # Extract text, mask, and label from the data\n","    text = data['text']\n","    mask = data['mask']\n","    label = data['label']\n","\n","    # Convert text and mask tensors to numpy arrays\n","    text = [t.numpy() for t in text]\n","    mask = [t.numpy() for t in mask]\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    text = torch.tensor(text)\n","    mask = torch.tensor(mask)\n","    label = torch.tensor(label)\n","\n","    return text, mask, label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["30a79de3beea41f69ca6d8b7fe527a1c","dc2962c8797a42fa853248f9af82dabb","c8dda0b8af5e48fda855907ec823a924","b7aaa6a4673742b0bd467a3b4f880993","23efab7d9da946e5b9780b53a6a7a47c","346123cbe9a44612bd24d92fd9d4ea2c","42fe9b2bb7c847cdbb3b5dcccb23e644","40f2b7c657f04566bb71ecba4c875b03","fd71d3247736467ebbe1283680f1e850","9e768fe9570b463b875434e553fe2877","1816d4d262f54a03a65118bea534ffd0","203f689f1e794cf186862a1fe6173fb1","8c232dbfebb04a92badd225de1521720","b78b70cbfb5045958896a2b71637eab9","2c9684a3b182477abb68dd2c87552cf9","71f742ca629a4f72ab10f5c59a12b035","5b1d37e500574dcfb715aad6d00cf280","968cc6711abf442fb0a3c1002384509a","6cab9312541a419d9b05cac6e1dc90cd","ca3c8b4e783b4b939ac54d9ed113db7b","0f288caaa309445cab55c44fd4794272","466b7ad6155f4a648b6fd5e74cf87975"]},"executionInfo":{"elapsed":4070,"status":"ok","timestamp":1709823681336,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"6BasTmByTlLF","outputId":"7c2e3939-bdee-4c8d-e87f-c98569224a33"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30a79de3beea41f69ca6d8b7fe527a1c","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"203f689f1e794cf186862a1fe6173fb1","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["BERT_Classifier(\n","  (encoder): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (fc): Linear(in_features=768, out_features=42, bias=True)\n","  (criterion): CrossEntropyLoss()\n",")"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["class BERT_Classifier(nn.Module):\n","    def __init__(self, label_num):\n","        super().__init__()\n","        # Initialize the BERT encoder from pre-trained weights\n","        model_directory = '/content/Drive/MyDrive/COMP61332_text_mining/RE/Tacred/Bert_RE/span-bert/spanbert-base-cased'\n","\n","        # Load the tokenizer and model\n","        self.encoder = BertModel.from_pretrained(model_directory)\n","        # Dropout layer to prevent overfitting\n","        self.dropout = nn.Dropout(0.1, inplace=False)\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(768, label_num)  # 768 is the hidden size of BERT\n","        # Cross-entropy loss criterion\n","        self.criterion = nn.CrossEntropyLoss()\n","\n","    def forward(self, x, attention_mask, label=None):\n","        # Pass the input through the BERT encoder\n","        x = self.encoder(x, attention_mask=attention_mask)[0]  # Output is tuple (last_hidden_state, pooler_output), we take the last_hidden_state\n","        # Take only the first token's output (CLS token)\n","        x = x[:, 0, :]\n","        # Apply dropout\n","        x = self.dropout(x)\n","        # Pass through the fully connected layer\n","        x = self.fc(x)\n","        # If label is not provided, return logits only\n","        if label is None:\n","            return None, x\n","        else:\n","            # Calculate the cross-entropy loss and return both loss and logits\n","            return self.criterion(x, label), x\n","\n","labels_num=len(rel2id)\n","# print(labels_num)\n","model = BERT_Classifier(labels_num)\n","# Tell pytorch to run this model on the GPU.\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAyTlPuuU_7m"},"outputs":[],"source":["def train(net, train_dataset, dev_dataset, num_epochs, learning_rate, batch_size):\n","\n","    print('Training...')\n","\n","    # Set the network to training mode\n","    net.train()\n","\n","    # Define the optimizer\n","    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n","\n","    # Create a data loader for training data\n","    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n","\n","    for epoch in range(num_epochs):\n","        correct = 0\n","        total = 0\n","        iter = 0\n","        all_pred = []\n","        all_true = []\n","\n","        # Initialize tqdm to show progress bar\n","        progress_bar = tqdm(train_iter, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch')\n","\n","        for text, mask, y in progress_bar:\n","            iter += 1\n","            optimizer.zero_grad()\n","\n","            # If the batch size is not equal to the specified batch size, break the loop\n","            if text.size(0) != batch_size:\n","                break\n","\n","            # Reshape text and mask tensors\n","            text = text.reshape(batch_size, -1)\n","            mask = mask.reshape(batch_size, -1)\n","\n","            # Move tensors to GPU if available\n","            if USE_CUDA:\n","                text = text.cuda()\n","                mask = mask.cuda()\n","                y = y.cuda()\n","\n","            # Forward pass\n","            loss, logits = net(text, mask, y)\n","\n","            # Backpropagation\n","            loss.backward()\n","            optimizer.step()\n","\n","            # Compute accuracy\n","            _, predicted = torch.max(logits.data, 1)\n","            total += text.size(0)\n","            correct += predicted.data.eq(y.data).cpu().sum()\n","\n","            # Collect predictions and true labels\n","            all_pred.extend(predicted.cpu().numpy())\n","            all_true.extend(y.cpu().numpy())\n","\n","            # Update progress bar\n","            progress_bar.set_postfix({'loss': loss.item(), 'accuracy': correct.item() / total})\n","\n","        # After the end of each epoch, compute metrics\n","        accuracy = correct.cpu().numpy().tolist()/total\n","        loss = loss.detach().cpu()\n","\n","        # Compute F1 scores\n","        macro_f1 = f1_score(all_true, all_pred, average='macro')\n","        micro_f1 = f1_score(all_true, all_pred, average='micro')\n","        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n","\n","        # Print metrics\n","        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n","        print(f\"Loss: {loss.mean().numpy().tolist()}, Accuracy: {accuracy}\")\n","        print(f\"Macro F1: {macro_f1}, Micro F1: {micro_f1}, Weighted F1: {weighted_f1}\")\n","\n","        print()\n","\n","        print(\"Validation...\")\n","\n","        dev_acc, dev_macro_f1, dev_micro_f1, dev_weighted_f1, _, _ = eval(net, dev_dataset, batch_size)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRLfrySsMHDX"},"outputs":[],"source":["\n","def eval(net, dataset, batch_size):\n","    # Set the network to evaluation mode\n","    net.eval()\n","\n","    # Create an iterator for the evaluation dataset\n","    eval_iter = DataLoader(dataset, batch_size, shuffle=False)\n","\n","    # Lists to store predictions and true labels\n","    all_pred = []\n","    all_true = []\n","\n","    # Lists to store evaluation metrics\n","    acc_list = []\n","    macro_f1_list = []\n","    micro_f1_list = []\n","    weighted_f1_list = []\n","    precision_list = []  # List for precision scores\n","    recall_list = []     # List for recall scores\n","\n","    with torch.no_grad():\n","        correct = 0  # Counter for correctly classified samples\n","        total = 0    # Counter for total samples\n","        # Progress bar for visualization during evaluation\n","        progress_bar = tqdm(eval_iter, desc='Evaluation', unit='batch')\n","\n","        for text, mask, y in progress_bar:\n","            if text.size(0) != batch_size:\n","                break\n","\n","            text = text.reshape(batch_size, -1)\n","            mask = mask.reshape(batch_size, -1)\n","\n","            if USE_CUDA:\n","                text, mask, y = text.cuda(), mask.cuda(), y.cuda()\n","\n","            outputs = net(text, mask)\n","            loss, logits = outputs if isinstance(outputs, tuple) else (None, outputs)\n","\n","            _, predicted = torch.max(logits, 1)\n","            total += y.size(0)\n","            correct += (predicted == y).sum().item()\n","\n","            all_pred.extend(predicted.cpu().numpy())\n","            all_true.extend(y.cpu().numpy())\n","\n","        # Calculate overall accuracy and F1 scores\n","        acc = correct / total\n","        macro_f1 = f1_score(all_true, all_pred, average='macro')\n","        micro_f1 = f1_score(all_true, all_pred, average='micro')\n","        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n","\n","        # Calculate precision and recall\n","        precision_macro = precision_score(all_true, all_pred, average='macro')\n","        recall_macro = recall_score(all_true, all_pred, average='macro')\n","        precision_micro = precision_score(all_true, all_pred, average='micro')\n","        recall_micro = recall_score(all_true, all_pred, average='micro')\n","        precision_weighted = precision_score(all_true, all_pred, average='weighted')\n","        recall_weighted = recall_score(all_true, all_pred, average='weighted')\n","\n","        # Append metrics to respective lists\n","        acc_list.append(acc)\n","        macro_f1_list.append(macro_f1)\n","        micro_f1_list.append(micro_f1)\n","        weighted_f1_list.append(weighted_f1)\n","        precision_list.append((precision_macro, precision_micro, precision_weighted))\n","        recall_list.append((recall_macro, recall_micro, recall_weighted))\n","\n","        # Print evaluation results\n","        print(f\"Eval Result: Acc: {acc:.4f}, Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}, Weighted F1: {weighted_f1:.4f}\")\n","        print(f\"Precision (Macro, Micro, Weighted): {precision_macro:.4f}, {precision_micro:.4f}, {precision_weighted:.4f}\")\n","        print(f\"Recall (Macro, Micro, Weighted): {recall_macro:.4f}, {recall_micro:.4f}, {recall_weighted:.4f}\")\n","\n","        # Return evaluation metrics\n","        return acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GxJrdSDTv07h"},"outputs":[],"source":["# Load data from JSON file\n","train_data = load_tacred_dataset('/content/Drive/MyDrive/COMP61332_text_mining/RE/Tacred/json/train.json')\n","dev_data = load_tacred_dataset('/content/Drive/MyDrive/COMP61332_text_mining/RE/Tacred/json/dev.json')\n","test_data = load_tacred_dataset('/content/Drive/MyDrive/COMP61332_text_mining/RE/Tacred/json/test.json')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1709823692735,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"MN2PJFW2v1HN","outputId":"83d03604-169c-47b8-90cd-ff967ad3eb64"},"outputs":[{"name":"stdout","output_type":"stream","text":["68124\n","22631\n","15509\n","{'rel': 'no_relation', 'ent1': 'Forsberg', 'ent2': 'John D.', 'text': \"In 1983 , a year after the rally , [MASK] received the so-called `` genius award '' from the [MASK] and Catherine T. MacArthur Foundation .\"}\n"]}],"source":["train_info = prepare_data(train_data)\n","dev_info = prepare_data(dev_data)\n","test_info = prepare_data(test_data)\n","\n","print(len(train_info))\n","print(len(dev_info))\n","print(len(test_info))\n","print(train_info[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110178,"status":"ok","timestamp":1709823802909,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"zpwOfzpzOiEf","outputId":"2104841b-70f7-4964-eea8-21c51ba2b089"},"outputs":[{"name":"stdout","output_type":"stream","text":["68124\n","22631\n","15509\n"]}],"source":["train_data = process_relation_extraction_data(train_info, 128)\n","dev_data = process_relation_extraction_data(dev_info, 128)\n","test_data = process_relation_extraction_data(test_info, 128)\n","\n","\n","print(len(train_data['label']))\n","print(len(dev_data['label']))\n","print(len(test_data['label']))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5009,"status":"ok","timestamp":1709823807901,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"AR6bEgGCMHDY","outputId":"769d1b7c-7287-4820-c5fd-91ac7d07207c"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-11-846317e3e88f>:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  text = torch.tensor(text)\n"]},{"name":"stdout","output_type":"stream","text":["--train data--\n","torch.Size([68124, 1, 128])\n","torch.Size([68124, 1, 128])\n","torch.Size([68124])\n","--eval data--\n","torch.Size([22631, 1, 128])\n","torch.Size([22631, 1, 128])\n","torch.Size([22631])\n","--test data--\n","torch.Size([15509, 1, 128])\n","torch.Size([15509, 1, 128])\n","torch.Size([15509])\n"]}],"source":["# Preprocess train data\n","train_text, train_mask, train_label = convert_data_to_tensors(train_data)\n","print(\"--train data--\")\n","print(train_text.shape)\n","print(train_mask.shape)\n","print(train_label.shape)\n","\n","# Preprocess dev data\n","dev_text, dev_mask, dev_label = convert_data_to_tensors(dev_data)\n","print(\"--eval data--\")\n","print(dev_text.shape)\n","print(dev_mask.shape)\n","print(dev_label.shape)\n","\n","# Preprocess test data\n","test_text, test_mask, test_label = convert_data_to_tensors(test_data)\n","print(\"--test data--\")\n","print(test_text.shape)\n","print(test_mask.shape)\n","print(test_label.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5plRuzg4U_4u"},"outputs":[],"source":["train_dataset = torch.utils.data.TensorDataset(train_text,train_mask,train_label)\n","dev_dataset = torch.utils.data.TensorDataset(dev_text,dev_mask,dev_label)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":571233,"status":"ok","timestamp":1709832096681,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"l-ofVsb7V6O4","outputId":"428b5003-5060-4f6e-dac5-1631942e5f4a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training...\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 1/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.15batch/s, loss=1.59, accuracy=0.808]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","Loss: 1.5880838632583618, Accuracy: 0.8081825657894737\n","Macro F1: 0.021326175259683104, Micro F1: 0.8081825657894737, Weighted F1: 0.7233361765279224\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 2/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.15batch/s, loss=0.652, accuracy=0.813]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2/20\n","Loss: 0.6519356369972229, Accuracy: 0.813234257518797\n","Macro F1: 0.03054672386393424, Micro F1: 0.813234257518797, Weighted F1: 0.7381366476788387\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 3/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.15batch/s, loss=0.486, accuracy=0.822]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3/20\n","Loss: 0.48626071214675903, Accuracy: 0.8217075892857143\n","Macro F1: 0.04719642333743773, Micro F1: 0.8217075892857142, Weighted F1: 0.75883112211996\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 4/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.15batch/s, loss=0.988, accuracy=0.831]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4/20\n","Loss: 0.9884495139122009, Accuracy: 0.8309151785714286\n","Macro F1: 0.07078476587192864, Micro F1: 0.8309151785714286, Weighted F1: 0.7750408966710549\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 5/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.14batch/s, loss=1.01, accuracy=0.838]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5/20\n","Loss: 1.0083210468292236, Accuracy: 0.8376703477443609\n","Macro F1: 0.09639334222974634, Micro F1: 0.8376703477443609, Weighted F1: 0.7882306966107725\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 6/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.14batch/s, loss=0.356, accuracy=0.846]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6/20\n","Loss: 0.35592734813690186, Accuracy: 0.8460261983082706\n","Macro F1: 0.13039423169420367, Micro F1: 0.8460261983082706, Weighted F1: 0.8029327233943531\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 7/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.14batch/s, loss=0.497, accuracy=0.852]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7/20\n","Loss: 0.4967767000198364, Accuracy: 0.8523114426691729\n","Macro F1: 0.16211571725393803, Micro F1: 0.8523114426691729, Weighted F1: 0.814227465498961\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 8/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.14batch/s, loss=0.786, accuracy=0.856]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8/20\n","Loss: 0.7858227491378784, Accuracy: 0.8562323778195489\n","Macro F1: 0.18889154434448224, Micro F1: 0.8562323778195489, Weighted F1: 0.8226162331907334\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 9/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.14batch/s, loss=1.09, accuracy=0.862]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9/20\n","Loss: 1.092086672782898, Accuracy: 0.862297344924812\n","Macro F1: 0.23035536223637187, Micro F1: 0.862297344924812, Weighted F1: 0.8330767883430494\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 10/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.14batch/s, loss=0.521, accuracy=0.867]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10/20\n","Loss: 0.5207123756408691, Accuracy: 0.8667910009398496\n","Macro F1: 0.25257298942988904, Micro F1: 0.8667910009398496, Weighted F1: 0.8401298898236669\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 11/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.14batch/s, loss=0.35, accuracy=0.87]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11/20\n","Loss: 0.349540114402771, Accuracy: 0.8704769736842105\n","Macro F1: 0.28173055254308155, Micro F1: 0.8704769736842105, Weighted F1: 0.8464835639130407\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 12/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.14batch/s, loss=0.769, accuracy=0.875]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12/20\n","Loss: 0.7690846920013428, Accuracy: 0.8754258693609023\n","Macro F1: 0.30570507033888344, Micro F1: 0.8754258693609023, Weighted F1: 0.8536656836186413\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 13/20: 100%|█████████▉| 2128/2129 [06:53<00:00,  5.14batch/s, loss=0.288, accuracy=0.879]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13/20\n","Loss: 0.2881782650947571, Accuracy: 0.8790677866541353\n","Macro F1: 0.3325770748167101, Micro F1: 0.8790677866541353, Weighted F1: 0.8593821712086258\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 14/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.14batch/s, loss=0.626, accuracy=0.883]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14/20\n","Loss: 0.6259258389472961, Accuracy: 0.8829593515037594\n","Macro F1: 0.3521456624234536, Micro F1: 0.8829593515037595, Weighted F1: 0.8650230978777146\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 15/20: 100%|█████████▉| 2128/2129 [06:55<00:00,  5.13batch/s, loss=0.357, accuracy=0.887]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15/20\n","Loss: 0.3573543429374695, Accuracy: 0.8865572133458647\n","Macro F1: 0.3788103456200234, Micro F1: 0.8865572133458647, Weighted F1: 0.8702560490379867\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 16/20: 100%|█████████▉| 2128/2129 [06:55<00:00,  5.12batch/s, loss=0.509, accuracy=0.892]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16/20\n","Loss: 0.508627712726593, Accuracy: 0.8916676456766918\n","Macro F1: 0.40188285386568223, Micro F1: 0.8916676456766918, Weighted F1: 0.8767077354990487\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 17/20: 100%|█████████▉| 2128/2129 [06:55<00:00,  5.13batch/s, loss=0.405, accuracy=0.895]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 17/20\n","Loss: 0.4049725830554962, Accuracy: 0.8950746005639098\n","Macro F1: 0.4234565202645503, Micro F1: 0.8950746005639099, Weighted F1: 0.8815383744497707\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 18/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.13batch/s, loss=0.267, accuracy=0.899]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 18/20\n","Loss: 0.26676639914512634, Accuracy: 0.8987165178571429\n","Macro F1: 0.4479076137337033, Micro F1: 0.8987165178571429, Weighted F1: 0.8861701979259721\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 19/20: 100%|█████████▉| 2128/2129 [06:54<00:00,  5.13batch/s, loss=0.606, accuracy=0.902]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 19/20\n","Loss: 0.6059044599533081, Accuracy: 0.9015654370300752\n","Macro F1: 0.4679489214784599, Micro F1: 0.9015654370300752, Weighted F1: 0.8902703457481905\n"]},{"name":"stderr","output_type":"stream","text":["Epoch 20/20: 100%|█████████▉| 2128/2129 [06:55<00:00,  5.12batch/s, loss=0.405, accuracy=0.905]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20/20\n","Loss: 0.4049081802368164, Accuracy: 0.904561207706767\n","Macro F1: 0.48793835432587235, Micro F1: 0.904561207706767, Weighted F1: 0.8938942004266996\n"]}],"source":["train(model, train_dataset, dev_dataset, 20, 0.002, 32)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":456,"status":"ok","timestamp":1709762788202,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"U6hEGxijaokN","outputId":"f6bc39f9-cd4f-4c62-cba0-3b552b300a11"},"outputs":[{"name":"stdout","output_type":"stream","text":["entire Model saved successfully.\n"]}],"source":["torch.save(model, 'span-bert_best_model_withmask.pth')\n","print(\"entire Model saved successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttbzHc3_Sm47"},"outputs":[],"source":["import json\n","\n","# grid parameter\n","batch_sizes = [32, 64, 128]\n","learning_rates = [0.02, 0.002, 0.0002]\n","num_epochs = [15, 20, 25]\n","\n","results = []\n","\n","# grid search\n","for batch_size in batch_sizes:\n","    for lr in learning_rates:\n","        for epoch in num_epochs:\n","            print(f\"Training with batch size = {batch_size}, lr = {lr}, epoch = {epoch}\")\n","            # set to initial state\n","            model = BERT_Classifier(labels_num)\n","            # Tell pytorch to run this model on the GPU.\n","            if torch.cuda.is_available():\n","                model.cuda()\n","            # run model\n","            train(model, train_dataset, dev_dataset, epoch, lr, batch_size)\n","\n","            # evaluate the model\n","            test_dataset = torch.utils.data.TensorDataset(test_text, test_mask, test_label)\n","            acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list = eval(model, test_dataset, batch_size)\n","\n","            # save result\n","            result = {\n","                'batch_size': batch_size,\n","                'lr': lr,\n","                'epoch': epoch,\n","                'acc_list': acc_list,\n","                'macro_f1_list': macro_f1_list,\n","                'micro_f1_list': micro_f1_list,\n","                'weighted_f1_list': weighted_f1_list,\n","                'precision_list': precision_list,\n","                'recall_list': recall_list\n","            }\n","            results.append(result)\n","\n","            # save in the file\n","            with open('grid_search_results.txt', 'a') as file:\n","                file.write(json.dumps(result) + '\\n')\n","\n","\n","for result in results:\n","    print(f\"Batch size: {result['batch_size']}, LR: {result['lr']}, Epoch: {result['epoch']}\")\n","    print(f\"Accuracy List: {result['acc_list']}\")\n","    print(f\"Macro F1 List: {result['macro_f1_list']}\")\n","    print(f\"Micro F1 List: {result['micro_f1_list']}\")\n","    print(f\"Weighted F1 List: {result['weighted_f1_list']}\")\n","    print(f\"Precision List: {result['precision_list']}\")\n","    print(f\"Recall List: {result['recall_list']}\")\n","    print(\"--------------------------------------------------\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33007,"status":"ok","timestamp":1709832129687,"user":{"displayName":"崔洋（Yang）","userId":"11100323723687542220"},"user_tz":0},"id":"XQtIP-GYMHDZ","outputId":"622ab003-ea04-4ce2-b2ad-87714e0f210a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Evaluation: 100%|█████████▉| 484/485 [00:32<00:00, 15.11batch/s]\n"]},{"name":"stdout","output_type":"stream","text":["Eval Result: Acc: 0.8224, Macro F1: 0.2634, Micro F1: 0.8224, Weighted F1: 0.7937\n","Precision (Macro, Micro, Weighted): 0.3915, 0.8224, 0.7833\n","Recall (Macro, Micro, Weighted): 0.2383, 0.8224, 0.8224\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["# use test dataset to evaluate the model \n","test_text, test_mask, test_label = convert_data_to_tensors(test_data)\n","test_dataset = torch.utils.data.TensorDataset(test_text, test_mask, test_label)\n","acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list= eval(model, test_dataset, 32)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","machine_shape":"hm","provenance":[{"file_id":"1rHn22kclw6MqTkuik9y9ntvS52xFC6TW","timestamp":1709672539353}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0f288caaa309445cab55c44fd4794272":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1816d4d262f54a03a65118bea534ffd0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"203f689f1e794cf186862a1fe6173fb1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8c232dbfebb04a92badd225de1521720","IPY_MODEL_b78b70cbfb5045958896a2b71637eab9","IPY_MODEL_2c9684a3b182477abb68dd2c87552cf9"],"layout":"IPY_MODEL_71f742ca629a4f72ab10f5c59a12b035"}},"23efab7d9da946e5b9780b53a6a7a47c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c9684a3b182477abb68dd2c87552cf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f288caaa309445cab55c44fd4794272","placeholder":"​","style":"IPY_MODEL_466b7ad6155f4a648b6fd5e74cf87975","value":" 440M/440M [00:01&lt;00:00, 282MB/s]"}},"30a79de3beea41f69ca6d8b7fe527a1c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc2962c8797a42fa853248f9af82dabb","IPY_MODEL_c8dda0b8af5e48fda855907ec823a924","IPY_MODEL_b7aaa6a4673742b0bd467a3b4f880993"],"layout":"IPY_MODEL_23efab7d9da946e5b9780b53a6a7a47c"}},"346123cbe9a44612bd24d92fd9d4ea2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40f2b7c657f04566bb71ecba4c875b03":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42fe9b2bb7c847cdbb3b5dcccb23e644":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"466b7ad6155f4a648b6fd5e74cf87975":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b1d37e500574dcfb715aad6d00cf280":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cab9312541a419d9b05cac6e1dc90cd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71f742ca629a4f72ab10f5c59a12b035":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c232dbfebb04a92badd225de1521720":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1d37e500574dcfb715aad6d00cf280","placeholder":"​","style":"IPY_MODEL_968cc6711abf442fb0a3c1002384509a","value":"model.safetensors: 100%"}},"968cc6711abf442fb0a3c1002384509a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e768fe9570b463b875434e553fe2877":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b78b70cbfb5045958896a2b71637eab9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6cab9312541a419d9b05cac6e1dc90cd","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca3c8b4e783b4b939ac54d9ed113db7b","value":440449768}},"b7aaa6a4673742b0bd467a3b4f880993":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e768fe9570b463b875434e553fe2877","placeholder":"​","style":"IPY_MODEL_1816d4d262f54a03a65118bea534ffd0","value":" 570/570 [00:00&lt;00:00, 44.2kB/s]"}},"c8dda0b8af5e48fda855907ec823a924":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40f2b7c657f04566bb71ecba4c875b03","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fd71d3247736467ebbe1283680f1e850","value":570}},"ca3c8b4e783b4b939ac54d9ed113db7b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc2962c8797a42fa853248f9af82dabb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_346123cbe9a44612bd24d92fd9d4ea2c","placeholder":"​","style":"IPY_MODEL_42fe9b2bb7c847cdbb3b5dcccb23e644","value":"config.json: 100%"}},"fd71d3247736467ebbe1283680f1e850":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
