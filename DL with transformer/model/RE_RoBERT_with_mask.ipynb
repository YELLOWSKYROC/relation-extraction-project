{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26601,
     "status": "ok",
     "timestamp": 1709751575685,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "Y5yej-1ovufz",
    "outputId": "a4436bac-50cf-41da-c478-cf15a07a8224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/Drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16011,
     "status": "ok",
     "timestamp": 1709751591692,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "nanNQZCEMHDO",
    "outputId": "f9827530-320e-4401-ea9c-7809070f19a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in /environment/miniconda3/lib/python3.10/site-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /environment/miniconda3/lib/python3.10/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /environment/miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting transformers\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b6/4d/fbe6d89fde59d8107f0a02816c4ac4542a8f9a85559fdf33c68282affcc1/transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m222.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ab/28/d4b691840d73126d4c9845f8a22dad033ac872509b6d3a0d93b456eef424/huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m190.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/81/8a/96a62ce98e8ff1b16db56fde3debc8a571f6b7ea42ee137eb0d995cdfa26/regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m238.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1c/5d/cf5e122ce4f1a29f165b2a69dc33d1ff30bce303343d58a54775ddba5d51/tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m225.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/ba/b2254fafc7f5fdc98a2fa4d5a5eeb029fbf9589ec87f2c230c3ac0a1dd53/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m209.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ad/30/2281c062222dc39328843bd1ddd30ff3005ef8e30b2fd09c4d2792766061/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m194.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.21.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.2\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tqdm in /environment/miniconda3/lib/python3.10/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6664,
     "status": "ok",
     "timestamp": 1709751598349,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "4NHSnwIXv0na"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import warnings\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1709751598350,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "yeH_1AYRDb8U",
    "outputId": "9ac815e3-8ff6-4f80-b7a2-91b00e77c527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_relation': 0, 'per:title': 1, 'org:top_members/employees': 2, 'org:country_of_headquarters': 3, 'per:parents': 4, 'per:age': 5, 'per:countries_of_residence': 6, 'per:children': 7, 'org:alternate_names': 8, 'per:charges': 9, 'per:cities_of_residence': 10, 'per:origin': 11, 'org:founded_by': 12, 'per:employee_of': 13, 'per:siblings': 14, 'per:alternate_names': 15, 'org:website': 16, 'per:religion': 17, 'per:stateorprovince_of_death': 18, 'org:parents': 19, 'org:subsidiaries': 20, 'per:other_family': 21, 'per:stateorprovinces_of_residence': 22, 'org:members': 23, 'per:cause_of_death': 24, 'org:member_of': 25, 'org:number_of_employees/members': 26, 'per:country_of_birth': 27, 'org:shareholders': 28, 'org:stateorprovince_of_headquarters': 29, 'per:city_of_death': 30, 'per:date_of_birth': 31, 'per:spouse': 32, 'org:city_of_headquarters': 33, 'per:date_of_death': 34, 'per:schools_attended': 35, 'org:political/religious_affiliation': 36, 'per:country_of_death': 37, 'org:founded': 38, 'per:stateorprovince_of_birth': 39, 'per:city_of_birth': 40, 'org:dissolved': 41}\n",
      "{0: 'no_relation', 1: 'per:title', 2: 'org:top_members/employees', 3: 'org:country_of_headquarters', 4: 'per:parents', 5: 'per:age', 6: 'per:countries_of_residence', 7: 'per:children', 8: 'org:alternate_names', 9: 'per:charges', 10: 'per:cities_of_residence', 11: 'per:origin', 12: 'org:founded_by', 13: 'per:employee_of', 14: 'per:siblings', 15: 'per:alternate_names', 16: 'org:website', 17: 'per:religion', 18: 'per:stateorprovince_of_death', 19: 'org:parents', 20: 'org:subsidiaries', 21: 'per:other_family', 22: 'per:stateorprovinces_of_residence', 23: 'org:members', 24: 'per:cause_of_death', 25: 'org:member_of', 26: 'org:number_of_employees/members', 27: 'per:country_of_birth', 28: 'org:shareholders', 29: 'org:stateorprovince_of_headquarters', 30: 'per:city_of_death', 31: 'per:date_of_birth', 32: 'per:spouse', 33: 'org:city_of_headquarters', 34: 'per:date_of_death', 35: 'per:schools_attended', 36: 'org:political/religious_affiliation', 37: 'per:country_of_death', 38: 'org:founded', 39: 'per:stateorprovince_of_birth', 40: 'per:city_of_birth', 41: 'org:dissolved'}\n"
     ]
    }
   ],
   "source": [
    "# List of relation types\n",
    "keys = ['no_relation', 'per:title', 'org:top_members/employees',\n",
    "        'org:country_of_headquarters', 'per:parents', 'per:age',\n",
    "        'per:countries_of_residence', 'per:children', 'org:alternate_names',\n",
    "        'per:charges', 'per:cities_of_residence', 'per:origin', 'org:founded_by',\n",
    "        'per:employee_of', 'per:siblings', 'per:alternate_names', 'org:website',\n",
    "        'per:religion', 'per:stateorprovince_of_death', 'org:parents',\n",
    "        'org:subsidiaries', 'per:other_family', 'per:stateorprovinces_of_residence',\n",
    "        'org:members', 'per:cause_of_death', 'org:member_of',\n",
    "        'org:number_of_employees/members', 'per:country_of_birth',\n",
    "        'org:shareholders', 'org:stateorprovince_of_headquarters',\n",
    "        'per:city_of_death', 'per:date_of_birth', 'per:spouse',\n",
    "        'org:city_of_headquarters', 'per:date_of_death', 'per:schools_attended',\n",
    "        'org:political/religious_affiliation', 'per:country_of_death',\n",
    "        'org:founded', 'per:stateorprovince_of_birth', 'per:city_of_birth',\n",
    "        'org:dissolved']\n",
    "\n",
    "# Assigning indices to the list elements and storing them in a dictionary\n",
    "rel2id = {key: idx for idx, key in enumerate(keys)}\n",
    "id2rel = {idx: key for key, idx in rel2id.items()}\n",
    "\n",
    "# Printing the dictionaries\n",
    "print(rel2id)\n",
    "print(id2rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1709751598350,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "Welvz8AbMHDR",
    "outputId": "e299c163-9fac-4be7-cdc6-fa77802cca95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 4090\n",
      "using GPU\n"
     ]
    }
   ],
   "source": [
    "# tell pytorch to use the gpu if available\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "if USE_CUDA:\n",
    "    print(\"using GPU\")\n",
    "else:\n",
    "    print(\"using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3132,
     "status": "ok",
     "timestamp": 1709751601470,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "R5bbj5oeMHDS",
    "outputId": "27e6b835-43f9-41b4-ad11-c9f2b27b2b9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "#RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
    "print('RoBERTa tokenizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1709751601471,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "6No8Wu6SN95j"
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    # Sets the seed for generating random numbers for the CPU.\n",
    "\n",
    "    # Sets the seed for generating random numbers for all GPUs.\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Sets the seed for generating random numbers with NumPy.\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Sets the seed for the built-in Python random module.\n",
    "    random.seed(seed)\n",
    "\n",
    "setup_seed(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709751601471,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "xTmRuAmIv04k"
   },
   "outputs": [],
   "source": [
    "def load_tacred_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load the TACRED dataset from a JSON file.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): The path to the JSON file containing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    dict: The loaded dataset.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709751601471,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "vLsXtBwYv1EN"
   },
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    # Get the number of instances in the data\n",
    "    n = len(data)\n",
    "\n",
    "    # Initialize an empty list to store processed data\n",
    "    info = []\n",
    "\n",
    "    # Iterate through each instance in the data\n",
    "    for i in range(n):\n",
    "        # Initialize an empty dictionary for storing processed information of a single data instance\n",
    "        single_data = {}\n",
    "\n",
    "        # Extract start and end indices of subject and object entities\n",
    "        ss = data[i][\"subj_start\"]\n",
    "        se = data[i][\"subj_end\"]\n",
    "        os = data[i][\"obj_start\"]\n",
    "        oe = data[i][\"obj_end\"]\n",
    "\n",
    "        # Extract subject and object tokens based on their start and end indices\n",
    "        subj = data[i]['token'][ss: se+1]\n",
    "        obj = data[i]['token'][os: oe+1]\n",
    "\n",
    "        temp = data[i]['token'].copy()\n",
    "\n",
    "        temp[ss: se+1] = ['[MASK]']\n",
    "        temp[os: oe+1] = ['[MASK]']\n",
    "\n",
    "        # Convert subject and object tokens into strings\n",
    "        ent1 = ' '.join(subj)\n",
    "        ent2 = ' '.join(obj)\n",
    "\n",
    "        # Extract relation label\n",
    "        rel = data[i][\"relation\"]\n",
    "\n",
    "        # Concatenate all tokens to form the original sentence\n",
    "        text = \" \".join(temp)\n",
    "\n",
    "        # Store processed information in the dictionary\n",
    "        single_data['rel'] = rel\n",
    "        single_data['ent1'] = ent1\n",
    "        single_data['ent2'] = ent2\n",
    "        single_data['text'] = text\n",
    "\n",
    "        # Append processed information of a single data instance to the list\n",
    "        info.append(single_data)\n",
    "\n",
    "    # Return the processed data\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1709751635227,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "pD96gM1FDb--"
   },
   "outputs": [],
   "source": [
    "def process_relation_extraction_data(info, max_length=64):\n",
    "    # Initialize a dictionary to store processed data\n",
    "    data = {}\n",
    "    data['label'] = []  # List to store relation labels\n",
    "    data['mask'] = []   # List to store attention masks\n",
    "    data['text'] = []   # List to store tokenized and padded texts\n",
    "\n",
    "    # Iterate through each instance in the provided 'info' data\n",
    "    for line in info:\n",
    "        # Check if the relation label is present in the 'rel2id' dictionary\n",
    "        if line['rel'] not in rel2id:\n",
    "            # If the relation label is not found, assign label 0 (for unknown relation)\n",
    "            data['label'].append(0)\n",
    "        else:\n",
    "            # If the relation label is found, assign its corresponding index\n",
    "            data['label'].append(rel2id[line['rel']])\n",
    "\n",
    "        # Concatenate subject, object, and text to form a single sentence\n",
    "        sent = line['ent1'] +'[SEP]'+line['ent2'] + '[SEP]' + line['text']\n",
    "\n",
    "        # Tokenize the concatenated sentence and add special tokens\n",
    "        indexed_tokens = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "        # Determine the available length of the tokenized sentence\n",
    "        avai_len = len(indexed_tokens)\n",
    "\n",
    "        # Pad the tokenized sentence with 0s to match the maximum length\n",
    "        while len(indexed_tokens) < max_length:\n",
    "            indexed_tokens.append(0)\n",
    "\n",
    "        # Trim the tokenized sentence to the maximum length\n",
    "        indexed_tokens = indexed_tokens[:max_length]\n",
    "\n",
    "        # Convert the tokenized sentence to a PyTorch tensor\n",
    "        indexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n",
    "\n",
    "        # Create an attention mask for the tokenized sentence\n",
    "        att_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\n",
    "        att_mask[0, :avai_len] = 1\n",
    "\n",
    "        # Append the tokenized and padded sentence, and its attention mask to the data dictionary\n",
    "        data['text'].append(indexed_tokens)\n",
    "        data['mask'].append(att_mask)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1709751641887,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "BTsJ6q_dMHDW"
   },
   "outputs": [],
   "source": [
    "def convert_data_to_tensors(data):\n",
    "    # Extract text, mask, and label from the data\n",
    "    text = data['text']\n",
    "    mask = data['mask']\n",
    "    label = data['label']\n",
    "\n",
    "    # Convert text and mask tensors to numpy arrays\n",
    "    text = [t.numpy() for t in text]\n",
    "    mask = [t.numpy() for t in mask]\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    text = torch.tensor(text)\n",
    "    mask = torch.tensor(mask)\n",
    "    label = torch.tensor(label)\n",
    "\n",
    "    return text, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1709751643817,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "6BasTmByTlLF",
    "outputId": "c968d852-37a5-437f-d7d6-364c6d5ba162"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RoBERTa_Classifier(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=768, out_features=42, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RoBERTa_Classifier(nn.Module):\n",
    "    def __init__(self, label_num):\n",
    "        super().__init__()\n",
    "        # Initialize the RoBERTa encoder from pre-trained weights\n",
    "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        # Dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(768, label_num)  # 768 is the hidden size of RoBERTa\n",
    "        # Cross-entropy loss criterion\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, attention_mask, label=None):\n",
    "        # Pass the input through the RoBERTa encoder\n",
    "        x = self.encoder(x, attention_mask=attention_mask)[0]  # Output is tuple (last_hidden_state, pooler_output), we take the last_hidden_state\n",
    "        # Take only the first token's output (CLS token)\n",
    "        x = x[:, 0, :]\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        # Pass through the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        # If label is not provided, return logits only\n",
    "        if label is None:\n",
    "            return None, x\n",
    "        else:\n",
    "            # Calculate the cross-entropy loss and return both loss and logits\n",
    "            return self.criterion(x, label), x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")        \n",
    "labels_num=len(rel2id)\n",
    "model = RoBERTa_Classifier(labels_num)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1709751647275,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "qAyTlPuuU_7m"
   },
   "outputs": [],
   "source": [
    "def train(net, train_dataset, dev_dataset, num_epochs, learning_rate, batch_size):\n",
    "\n",
    "    print('Training...')\n",
    "\n",
    "    # Set the network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a data loader for training data\n",
    "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        iter = 0\n",
    "        all_pred = []\n",
    "        all_true = []\n",
    "\n",
    "        # Initialize tqdm to show progress bar\n",
    "        progress_bar = tqdm(train_iter, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch')\n",
    "\n",
    "        for text, mask, y in progress_bar:\n",
    "            iter += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # If the batch size is not equal to the specified batch size, break the loop\n",
    "            if text.size(0) != batch_size:\n",
    "                break\n",
    "\n",
    "            # Reshape text and mask tensors\n",
    "            text = text.reshape(batch_size, -1)\n",
    "            mask = mask.reshape(batch_size, -1)\n",
    "\n",
    "            # Move tensors to GPU if available\n",
    "            if USE_CUDA:\n",
    "                text = text.cuda()\n",
    "                mask = mask.cuda()\n",
    "                y = y.cuda()\n",
    "\n",
    "            # Forward pass\n",
    "            loss, logits = net(text, mask, y)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += text.size(0)\n",
    "            correct += predicted.data.eq(y.data).cpu().sum()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_pred.extend(predicted.cpu().numpy())\n",
    "            all_true.extend(y.cpu().numpy())\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item(), 'accuracy': correct.item() / total})\n",
    "\n",
    "        # After the end of each epoch, compute metrics\n",
    "        accuracy = correct.cpu().numpy().tolist()/total\n",
    "        loss = loss.detach().cpu()\n",
    "\n",
    "        # Compute F1 scores\n",
    "        macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
    "        micro_f1 = f1_score(all_true, all_pred, average='micro')\n",
    "        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        print(f\"Loss: {loss.mean().numpy().tolist()}, Accuracy: {accuracy}\")\n",
    "        print(f\"Macro F1: {macro_f1}, Micro F1: {micro_f1}, Weighted F1: {weighted_f1}\")\n",
    "\n",
    "        # print()\n",
    "\n",
    "        # print(\"Validation...\")\n",
    "\n",
    "        # dev_acc, dev_macro_f1, dev_micro_f1, dev_weighted_f1, _, _ = eval(net, dev_dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1709751650430,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "RRLfrySsMHDX"
   },
   "outputs": [],
   "source": [
    "\n",
    "def eval(net, dataset, batch_size):\n",
    "    # Set the network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Create an iterator for the evaluation dataset\n",
    "    eval_iter = DataLoader(dataset, batch_size, shuffle=False)\n",
    "\n",
    "    # Lists to store predictions and true labels\n",
    "    all_pred = []\n",
    "    all_true = []\n",
    "\n",
    "    # Lists to store evaluation metrics\n",
    "    acc_list = []\n",
    "    macro_f1_list = []\n",
    "    micro_f1_list = []\n",
    "    weighted_f1_list = []\n",
    "    precision_list = []  # List for precision scores\n",
    "    recall_list = []     # List for recall scores\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0  # Counter for correctly classified samples\n",
    "        total = 0    # Counter for total samples\n",
    "        # Progress bar for visualization during evaluation\n",
    "        progress_bar = tqdm(eval_iter, desc='Evaluation', unit='batch')\n",
    "\n",
    "        for text, mask, y in progress_bar:\n",
    "            if text.size(0) != batch_size:\n",
    "                break\n",
    "\n",
    "            text = text.reshape(batch_size, -1)\n",
    "            mask = mask.reshape(batch_size, -1)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                text, mask, y = text.cuda(), mask.cuda(), y.cuda()\n",
    "\n",
    "            outputs = net(text, mask)\n",
    "            loss, logits = outputs if isinstance(outputs, tuple) else (None, outputs)\n",
    "\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "\n",
    "            all_pred.extend(predicted.cpu().numpy())\n",
    "            all_true.extend(y.cpu().numpy())\n",
    "\n",
    "        # Calculate overall accuracy and F1 scores\n",
    "        acc = correct / total\n",
    "        macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
    "        micro_f1 = f1_score(all_true, all_pred, average='micro')\n",
    "        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision_macro = precision_score(all_true, all_pred, average='macro')\n",
    "        recall_macro = recall_score(all_true, all_pred, average='macro')\n",
    "        precision_micro = precision_score(all_true, all_pred, average='micro')\n",
    "        recall_micro = recall_score(all_true, all_pred, average='micro')\n",
    "        precision_weighted = precision_score(all_true, all_pred, average='weighted')\n",
    "        recall_weighted = recall_score(all_true, all_pred, average='weighted')\n",
    "\n",
    "        # Append metrics to respective lists\n",
    "        acc_list.append(acc)\n",
    "        macro_f1_list.append(macro_f1)\n",
    "        micro_f1_list.append(micro_f1)\n",
    "        weighted_f1_list.append(weighted_f1)\n",
    "        precision_list.append((precision_macro, precision_micro, precision_weighted))\n",
    "        recall_list.append((recall_macro, recall_micro, recall_weighted))\n",
    "\n",
    "        # Print evaluation results\n",
    "        print(f\"Eval Result: Acc: {acc:.4f}, Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}, Weighted F1: {weighted_f1:.4f}\")\n",
    "        print(f\"Precision (Macro, Micro, Weighted): {precision_macro:.4f}, {precision_micro:.4f}, {precision_weighted:.4f}\")\n",
    "        print(f\"Recall (Macro, Micro, Weighted): {recall_macro:.4f}, {recall_micro:.4f}, {recall_weighted:.4f}\")\n",
    "\n",
    "        # Return evaluation metrics\n",
    "        return acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 11829,
     "status": "ok",
     "timestamp": 1709751666892,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "GxJrdSDTv07h"
   },
   "outputs": [],
   "source": [
    "# Load data from JSON file\n",
    "train_data = load_tacred_dataset('/home/featurize/data/TACRED/train.json')\n",
    "dev_data = load_tacred_dataset('/home/featurize/data/TACRED/dev.json')\n",
    "test_data = load_tacred_dataset('/home/featurize/data/TACRED/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709751666893,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "MN2PJFW2v1HN",
    "outputId": "ea2edb1f-4458-458f-94d0-1d5d740420d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68124\n",
      "22631\n",
      "15509\n",
      "{'rel': 'no_relation', 'ent1': 'Forsberg', 'ent2': 'John D.', 'text': \"In 1983 , a year after the rally , [MASK] received the so-called `` genius award '' from the [MASK] and Catherine T. MacArthur Foundation .\"}\n"
     ]
    }
   ],
   "source": [
    "train_info = prepare_data(train_data)\n",
    "dev_info = prepare_data(dev_data)\n",
    "test_info = prepare_data(test_data)\n",
    "\n",
    "print(len(train_info))\n",
    "print(len(dev_info))\n",
    "print(len(test_info))\n",
    "print(train_info[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127738,
     "status": "ok",
     "timestamp": 1709752471106,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "zpwOfzpzOiEf",
    "outputId": "2059942a-d281-4b67-ceb8-8c18b6dc146c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68124\n",
      "22631\n",
      "15509\n"
     ]
    }
   ],
   "source": [
    "train_data = process_relation_extraction_data(train_info, 128)\n",
    "dev_data = process_relation_extraction_data(dev_info, 128)\n",
    "test_data = process_relation_extraction_data(test_info, 128)\n",
    "\n",
    "\n",
    "print(len(train_data['label']))\n",
    "print(len(dev_data['label']))\n",
    "print(len(test_data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6176,
     "status": "ok",
     "timestamp": 1709752477280,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "AR6bEgGCMHDY",
    "outputId": "7cd3a7bf-68db-43d8-bc18-d1e5f13a3c34"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20642/3242763015.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  text = torch.tensor(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--train data--\n",
      "torch.Size([68124, 1, 128])\n",
      "torch.Size([68124, 1, 128])\n",
      "torch.Size([68124])\n",
      "--eval data--\n",
      "torch.Size([22631, 1, 128])\n",
      "torch.Size([22631, 1, 128])\n",
      "torch.Size([22631])\n",
      "--test data--\n",
      "torch.Size([15509, 1, 128])\n",
      "torch.Size([15509, 1, 128])\n",
      "torch.Size([15509])\n"
     ]
    }
   ],
   "source": [
    "# Preprocess train data\n",
    "train_text, train_mask, train_label = convert_data_to_tensors(train_data)\n",
    "print(\"--train data--\")\n",
    "print(train_text.shape)\n",
    "print(train_mask.shape)\n",
    "print(train_label.shape)\n",
    "\n",
    "# Preprocess dev data\n",
    "dev_text, dev_mask, dev_label = convert_data_to_tensors(dev_data)\n",
    "print(\"--eval data--\")\n",
    "print(dev_text.shape)\n",
    "print(dev_mask.shape)\n",
    "print(dev_label.shape)\n",
    "\n",
    "# Preprocess test data\n",
    "test_text, test_mask, test_label = convert_data_to_tensors(test_data)\n",
    "print(\"--test data--\")\n",
    "print(test_text.shape)\n",
    "print(test_mask.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709752477280,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "5plRuzg4U_4u"
   },
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_text,train_mask,train_label)\n",
    "dev_dataset = torch.utils.data.TensorDataset(dev_text,dev_mask,dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-ofVsb7V6O4",
    "outputId": "afb3e03a-f11d-4aba-8252-fa01693d27e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.59batch/s, loss=0.635, accuracy=0.831] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "Loss: 0.6349697113037109, Accuracy: 0.8310870331219169\n",
      "Macro F1: 0.1148647140638592, Micro F1: 0.8310870331219169, Weighted F1: 0.7882030084862661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.62batch/s, loss=0.0896, accuracy=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "Loss: 0.08964329212903976, Accuracy: 0.866014799154334\n",
      "Macro F1: 0.3221312394994451, Micro F1: 0.866014799154334, Weighted F1: 0.846426814901049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|█████████▉| 4257/4258 [02:50<00:00, 24.95batch/s, loss=0.236, accuracy=0.882] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "Loss: 0.23647284507751465, Accuracy: 0.881915081042988\n",
      "Macro F1: 0.429078798721458, Micro F1: 0.881915081042988, Weighted F1: 0.8693259069463032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.57batch/s, loss=0.232, accuracy=0.889] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "Loss: 0.23202520608901978, Accuracy: 0.8891237961005403\n",
      "Macro F1: 0.4929335163313511, Micro F1: 0.8891237961005404, Weighted F1: 0.8796125546384035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.52batch/s, loss=0.414, accuracy=0.896] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "Loss: 0.4135894179344177, Accuracy: 0.8960095137420718\n",
      "Macro F1: 0.5319202209061303, Micro F1: 0.8960095137420718, Weighted F1: 0.888483459841132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.71batch/s, loss=0.358, accuracy=0.902] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15\n",
      "Loss: 0.35798826813697815, Accuracy: 0.9018968757340851\n",
      "Macro F1: 0.5638104470808465, Micro F1: 0.9018968757340851, Weighted F1: 0.8954363111731823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.62batch/s, loss=0.279, accuracy=0.906] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "Loss: 0.2790214717388153, Accuracy: 0.9058756166314306\n",
      "Macro F1: 0.5909264787528045, Micro F1: 0.9058756166314306, Weighted F1: 0.9001279034142783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.55batch/s, loss=0.0747, accuracy=0.909] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "Loss: 0.07469341903924942, Accuracy: 0.9086210946676063\n",
      "Macro F1: 0.609322527621792, Micro F1: 0.9086210946676063, Weighted F1: 0.9038415777561437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.49batch/s, loss=0.559, accuracy=0.913]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "Loss: 0.558610737323761, Accuracy: 0.9130109231853418\n",
      "Macro F1: 0.6273287588699896, Micro F1: 0.9130109231853418, Weighted F1: 0.9086179426461172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.74batch/s, loss=0.0673, accuracy=0.916] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "Loss: 0.06729257106781006, Accuracy: 0.915859173126615\n",
      "Macro F1: 0.6472551146109, Micro F1: 0.915859173126615, Weighted F1: 0.9120516896900376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.48batch/s, loss=0.0541, accuracy=0.92]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "Loss: 0.054138537496328354, Accuracy: 0.9197791872210477\n",
      "Macro F1: 0.6669371750583504, Micro F1: 0.9197791872210477, Weighted F1: 0.9166061293446456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.62batch/s, loss=0.306, accuracy=0.922]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "Loss: 0.3060624301433563, Accuracy: 0.922084214235377\n",
      "Macro F1: 0.6764884546688139, Micro F1: 0.922084214235377, Weighted F1: 0.9191726844509179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|█████████▉| 4257/4258 [02:53<00:00, 24.59batch/s, loss=0.0248, accuracy=0.926] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "Loss: 0.024845732375979424, Accuracy: 0.9264299976509279\n",
      "Macro F1: 0.6950956727078037, Micro F1: 0.9264299976509279, Weighted F1: 0.9239417434475099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|█████████▉| 4257/4258 [02:52<00:00, 24.72batch/s, loss=0.306, accuracy=0.93]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "Loss: 0.3058245778083801, Accuracy: 0.9304087385482734\n",
      "Macro F1: 0.7225880768179801, Micro F1: 0.9304087385482734, Weighted F1: 0.9284947718525174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|█████████▉| 4257/4258 [02:55<00:00, 24.32batch/s, loss=0.101, accuracy=0.934]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "Loss: 0.10093909502029419, Accuracy: 0.9337121212121212\n",
      "Macro F1: 0.729940844101015, Micro F1: 0.9337121212121212, Weighted F1: 0.9318697662441596\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataset, dev_dataset, 15, 0.002, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "U6hEGxijaokN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entire Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, 'roberta_best_model_withmask.pth')\n",
    "torch.save(model.state_dict(), 'roberta_best_model_withmask2.pth')\n",
    "print(\"entire Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18350,
     "status": "ok",
     "timestamp": 1709511798943,
     "user": {
      "displayName": "崔洋（Yang）",
      "userId": "11100323723687542220"
     },
     "user_tz": 0
    },
    "id": "XQtIP-GYMHDZ",
    "outputId": "8d0aaf1f-cfbc-4c63-db86-4b2f9951dc3e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|█████████▉| 484/485 [00:12<00:00, 39.07batch/s]\n",
      "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Result: Acc: 0.8768, Macro F1: 0.5516, Micro F1: 0.8768, Weighted F1: 0.8744\n",
      "Precision (Macro, Micro, Weighted): 0.5683, 0.8768, 0.8746\n",
      "Recall (Macro, Micro, Weighted): 0.5573, 0.8768, 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_mask, test_label)\n",
    "acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list= eval(model, test_dataset, 32)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1rHn22kclw6MqTkuik9y9ntvS52xFC6TW",
     "timestamp": 1709672539353
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
