{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5yej-1ovufz"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1wqs131YWdh",
        "outputId": "db5c852e-7ce7-4daa-db35-8df522402c29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: torch in /environment/miniconda3/lib/python3.10/site-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /environment/miniconda3/lib/python3.10/site-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /environment/miniconda3/lib/python3.10/site-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions in /environment/miniconda3/lib/python3.10/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /environment/miniconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.0)\n",
            "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /environment/miniconda3/lib/python3.10/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.25.0)\n",
            "Requirement already satisfied: lit in /environment/miniconda3/lib/python3.10/site-packages (from triton==2.0.0->torch) (15.0.7)\n",
            "Requirement already satisfied: numpy in /environment/miniconda3/lib/python3.10/site-packages (from torchvision) (1.24.1)\n",
            "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /environment/miniconda3/lib/python3.10/site-packages (from torchvision) (9.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.10/site-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: transformers in /environment/miniconda3/lib/python3.10/site-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (1.24.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /environment/miniconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
            "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
            "Requirement already satisfied: tqdm in /environment/miniconda3/lib/python3.10/site-packages (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NHSnwIXv0na",
        "outputId": "c2c445de-1e05-4bb5-bb75-fac3136d0ca2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/environment/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import torch\n",
        "import json\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeH_1AYRDb8U"
      },
      "outputs": [],
      "source": [
        "# List of relation types\n",
        "keys = ['no_relation', 'per:title', 'org:top_members/employees',\n",
        "        'org:country_of_headquarters', 'per:parents', 'per:age',\n",
        "        'per:countries_of_residence', 'per:children', 'org:alternate_names',\n",
        "        'per:charges', 'per:cities_of_residence', 'per:origin', 'org:founded_by',\n",
        "        'per:employee_of', 'per:siblings', 'per:alternate_names', 'org:website',\n",
        "        'per:religion', 'per:stateorprovince_of_death', 'org:parents',\n",
        "        'org:subsidiaries', 'per:other_family', 'per:stateorprovinces_of_residence',\n",
        "        'org:members', 'per:cause_of_death', 'org:member_of',\n",
        "        'org:number_of_employees/members', 'per:country_of_birth',\n",
        "        'org:shareholders', 'org:stateorprovince_of_headquarters',\n",
        "        'per:city_of_death', 'per:date_of_birth', 'per:spouse',\n",
        "        'org:city_of_headquarters', 'per:date_of_death', 'per:schools_attended',\n",
        "        'org:political/religious_affiliation', 'per:country_of_death',\n",
        "        'org:founded', 'per:stateorprovince_of_birth', 'per:city_of_birth',\n",
        "        'org:dissolved']\n",
        "\n",
        "# Assigning indices to the list elements and storing them in a dictionary\n",
        "rel2id = {key: idx for idx, key in enumerate(keys)}\n",
        "id2rel = {idx: key for key, idx in rel2id.items()}\n",
        "\n",
        "# # Printing the dictionaries\n",
        "# print(rel2id)\n",
        "# print(id2rel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTOnNI52YWdj",
        "outputId": "c9d0f02f-a12d-4f7d-a4c4-b68880bd7fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA GeForce RTX 3090\n",
            "using GPU\n"
          ]
        }
      ],
      "source": [
        "# tell pytorch to use the gpu if available\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "if USE_CUDA:\n",
        "    print(\"using GPU\")\n",
        "else:\n",
        "    print(\"using CPU\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpmJNfPDYWdj",
        "outputId": "0e77b3e1-4d93-4d7d-dfc7-b4e21f36df9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RoBERTa tokenizer loaded\n"
          ]
        }
      ],
      "source": [
        "#RoBERTa tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "print('RoBERTa tokenizer loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6No8Wu6SN95j"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "By setting the same seed, it ensures that the same sequence of random numbers is generated\n",
        "each time the program is run, thus making the experimental results reproducible and aiding\n",
        "in debugging and verifying the robustness of the model.\n",
        "\"\"\"\n",
        "\n",
        "def setup_seed(seed):\n",
        "    # Sets the seed for generating random numbers for the CPU.\n",
        "\n",
        "    # Sets the seed for generating random numbers for all GPUs.\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Sets the seed for generating random numbers with NumPy.\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Sets the seed for the built-in Python random module.\n",
        "    random.seed(seed)\n",
        "\n",
        "setup_seed(44)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTmRuAmIv04k"
      },
      "outputs": [],
      "source": [
        "def load_tacred_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Load the TACRED dataset from a JSON file.\n",
        "\n",
        "    Args:\n",
        "    file_path (str): The path to the JSON file containing the dataset.\n",
        "\n",
        "    Returns:\n",
        "    dict: The loaded dataset.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data preprocessing function `prepare_data(data)` plays a key role in natural language processing tasks. This function takes raw data and converts it into an easy-to-process format. It goes through each data instance, extracts the start and end index of the entity, extracts the entity text from it, and then gets the relationship label. It then combines words from the original text into sentences and organizes this processed information into dictionary form. Ultimately, it adds the dictionary of each data instance to a list and returns the list as output. This function not only provides the function of data format conversion, but also facilitates subsequent model training and text analysis."
      ],
      "metadata": {
        "id": "vW9UTllGa-p0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLsXtBwYv1EN"
      },
      "outputs": [],
      "source": [
        "def prepare_data(data):\n",
        "    # Get the number of instances in the data\n",
        "    n = len(data)\n",
        "\n",
        "    # Initialize an empty list to store processed data\n",
        "    info = []\n",
        "\n",
        "    # Iterate through each instance in the data\n",
        "    for i in range(n):\n",
        "        # Initialize an empty dictionary for storing processed information of a single data instance\n",
        "        single_data = {}\n",
        "\n",
        "        # Extract start and end indices of subject and object entities\n",
        "        ss = data[i][\"subj_start\"]\n",
        "        se = data[i][\"subj_end\"]\n",
        "        os = data[i][\"obj_start\"]\n",
        "        oe = data[i][\"obj_end\"]\n",
        "\n",
        "        # Extract subject and object tokens based on their start and end indices\n",
        "        subj = data[i]['token'][ss: se+1]\n",
        "        obj = data[i]['token'][os: oe+1]\n",
        "\n",
        "        # Convert subject and object tokens into strings\n",
        "        ent1 = ' '.join(subj)\n",
        "        ent2 = ' '.join(obj)\n",
        "\n",
        "        # Extract relation label\n",
        "        rel = data[i][\"relation\"]\n",
        "\n",
        "        # Concatenate all tokens to form the original sentence\n",
        "        text = \" \".join(data[i][\"token\"])\n",
        "\n",
        "        # Store processed information in the dictionary\n",
        "        single_data['rel'] = rel\n",
        "        single_data['ent1'] = ent1\n",
        "        single_data['ent2'] = ent2\n",
        "        single_data['text'] = text\n",
        "\n",
        "        # Append processed information of a single data instance to the list\n",
        "        info.append(single_data)\n",
        "\n",
        "    # Return the processed data\n",
        "    return info"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `process_relation_extraction_data(info, max_length=64)` function plays a vital role in the relationship extraction task. This function is used to process raw data and prepare it into a format suitable for model input. First, the function traverses the provided data information and integrates entities, text information, and relationship tags. For each data instance, the function will splice the entity and text information into a sentence, and perform word segmentation and filling operations. Next, the sentences are tokenized by adding special tags. The function truncates or pads sentences according to the given maximum length to ensure consistent sentence lengths. Finally, the function adds the processed sentences and their corresponding attention masks to the output data. In this way, we are able to transform the raw data into a form suitable for model training and prepare it for further relationship extraction tasks."
      ],
      "metadata": {
        "id": "wjDaZbhcbClO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD96gM1FDb--"
      },
      "outputs": [],
      "source": [
        "def process_relation_extraction_data(info, max_length=64):\n",
        "    # Initialize a dictionary to store processed data\n",
        "    data = {}\n",
        "    data['label'] = []  # List to store relation labels\n",
        "    data['mask'] = []   # List to store attention masks\n",
        "    data['text'] = []   # List to store tokenized and padded texts\n",
        "\n",
        "    # Iterate through each instance in the provided 'info' data\n",
        "    for line in info:\n",
        "        # Check if the relation label is present in the 'rel2id' dictionary\n",
        "        if line['rel'] not in rel2id:\n",
        "            # If the relation label is not found, assign label 0 (for unknown relation)\n",
        "            data['label'].append(0)\n",
        "        else:\n",
        "            # If the relation label is found, assign its corresponding index\n",
        "            data['label'].append(rel2id[line['rel']])\n",
        "\n",
        "        # Concatenate subject, object, and text to form a single sentence\n",
        "        sent = line['ent1'] + line['ent2'] + line['text']\n",
        "\n",
        "        # Tokenize the concatenated sentence and add special tokens\n",
        "        indexed_tokens = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "        # Determine the available length of the tokenized sentence\n",
        "        avai_len = len(indexed_tokens)\n",
        "\n",
        "        # Pad the tokenized sentence with 0s to match the maximum length\n",
        "        while len(indexed_tokens) < max_length:\n",
        "            indexed_tokens.append(0)\n",
        "\n",
        "        # Trim the tokenized sentence to the maximum length\n",
        "        indexed_tokens = indexed_tokens[:max_length]\n",
        "\n",
        "        # Convert the tokenized sentence to a PyTorch tensor\n",
        "        indexed_tokens = torch.tensor(indexed_tokens).long().unsqueeze(0)  # (1, L)\n",
        "\n",
        "        # Create an attention mask for the tokenized sentence\n",
        "        att_mask = torch.zeros(indexed_tokens.size()).long()  # (1, L)\n",
        "        att_mask[0, :avai_len] = 1\n",
        "\n",
        "        # Append the tokenized and padded sentence, and its attention mask to the data dictionary\n",
        "        data['text'].append(indexed_tokens)\n",
        "        data['mask'].append(att_mask)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7mDxa4BYWdk"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_tensors(data):\n",
        "    # Extract text, mask, and label from the data\n",
        "    text = data['text']\n",
        "    mask = data['mask']\n",
        "    label = data['label']\n",
        "\n",
        "    # Convert text and mask tensors to numpy arrays\n",
        "    text = [t.numpy() for t in text]\n",
        "    mask = [t.numpy() for t in mask]\n",
        "\n",
        "    # Convert numpy arrays to PyTorch tensors\n",
        "    text = torch.tensor(text)\n",
        "    mask = torch.tensor(mask)\n",
        "    label = torch.tensor(label)\n",
        "\n",
        "    return text, mask, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `RoBERTa_Classifier` class implements a RoBERTa-based classifier for relation extraction tasks. The model utilizes pre-trained RoBERTa encoder to extract text features and perform classification through fully connected layers. During the initialization process, the pretrained RoBERTa model is loaded by calling the `from_pretrained` method, and a fully connected layer containing `label_num` output categories is defined. The model also includes a dropout layer to prevent overfitting and uses cross-entropy loss as the loss function for classification.\n",
        "\n",
        "In the forward propagation process, the input data passes through the RoBERTa encoder to obtain text feature representation, and then is classified through the fully connected layer. The final output is the class probability distribution predicted by the model. If a label is provided, the model computes the cross-entropy loss and returns the loss value and the predicted probability distribution; otherwise, only the predicted probability distribution is returned.\n",
        "\n",
        "When in use, we can initialize the model according to specific tasks and number of labels, and move it to a designated computing device (such as GPU). Use `labels_num=len(rel2id)` to get the number of relationship labels and create the corresponding model instance."
      ],
      "metadata": {
        "id": "Koxj0QcrbIrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BasTmByTlLF",
        "outputId": "72f4b890-3f2a-48e6-9e40-770858b634a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RoBERTa_Classifier(\n",
              "  (encoder): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc): Linear(in_features=768, out_features=42, bias=True)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class RoBERTa_Classifier(nn.Module):\n",
        "    def __init__(self, label_num):\n",
        "        super().__init__()\n",
        "        # Initialize the RoBERTa encoder from pre-trained weights\n",
        "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(768, label_num)  # 768 is the hidden size of RoBERTa\n",
        "        # Cross-entropy loss criterion\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, attention_mask, label=None):\n",
        "        # Pass the input through the RoBERTa encoder\n",
        "        x = self.encoder(x, attention_mask=attention_mask)[0]  # Output is tuple (last_hidden_state, pooler_output), we take the last_hidden_state\n",
        "        # Take only the first token's output (CLS token)\n",
        "        x = x[:, 0, :]\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # Pass through the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        # If label is not provided, return logits only\n",
        "        if label is None:\n",
        "            return None, x\n",
        "        else:\n",
        "            # Calculate the cross-entropy loss and return both loss and logits\n",
        "            return self.criterion(x, label), x\n",
        "\n",
        "labels_num=len(rel2id)\n",
        "model = RoBERTa_Classifier(labels_num)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `train` function is an important tool for model training. It receives a neural network model, a training data set, a validation data set, and some hyperparameters such as the number of training epochs, learning rate, and batch size. During training, this function sets the model to training mode and uses stochastic gradient descent (SGD) as the optimizer. It iterates over the training data set, performing forward propagation, computing loss, back propagation, and parameter updates in each batch. At the same time, it monitors the loss and accuracy of the model in real time, and uses the tqdm library to display the training progress. At the end of each cycle, this function calculates the accuracy, loss, F1 score and other evaluation indicators of the model on the training set and validation set, and prints out these indicators.\n",
        "\n",
        "Additionally, at the end of each epoch, the function calls the `eval` function to evaluate the model on the validation set. After training is completed, the model status will be saved to a .pth file for subsequent model application or further training. Overall, the `train` function is a core tool for completing model training and monitoring performance.\n",
        "\n",
        "In the `train` function, in addition to monitoring the loss and accuracy of the model, the F1 score is also used as an evaluation metric for model performance. At the end of each training cycle, the model's accuracy, loss, and F1 score under three different weighting methods on the training set and validation set were calculated: macro average (macro), micro average (micro), and weighted average (weighted ).\n",
        "\n",
        "The F1 score is an indicator that combines the precision and recall of the model. It comprehensively evaluates the imbalanced class distribution and classifier effect. It is calculated as the harmonic mean of precision and recall, which can be used to evaluate the performance of the model in multi-classification tasks. During this training process, three different ways of calculating F1 scores were used to comprehensively evaluate the model's performance on different categories.\n",
        "\n",
        "Specifically, when calculating these evaluation indicators, the `f1_score` function is used, in which the `average` parameter is set to `'macro'`, `'micro'` and `'weighted'` respectively to calculate the macro average, micro Average and weighted average F1 scores. These F1 scores provide an assessment of model performance at different levels, helping to better understand how the model performs in classification tasks."
      ],
      "metadata": {
        "id": "_0lXtivAbJgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAyTlPuuU_7m"
      },
      "outputs": [],
      "source": [
        "def train(net, train_dataset, dev_dataset, num_epochs, learning_rate, batch_size):\n",
        "\n",
        "    # Set the network to training mode\n",
        "    net.train()\n",
        "\n",
        "    # Define the optimizer\n",
        "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0)\n",
        "\n",
        "    # Create a data loader for training data\n",
        "    train_iter = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
        "\n",
        "    # Lists to store metrics\n",
        "    epoch_losses = []\n",
        "    epoch_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print('Training...')\n",
        "\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        iter = 0\n",
        "        all_pred = []\n",
        "        all_true = []\n",
        "\n",
        "        # Initialize tqdm to show progress bar\n",
        "        progress_bar = tqdm(train_iter, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch')\n",
        "\n",
        "        for text, mask, y in progress_bar:\n",
        "            iter += 1\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # If the batch size is not equal to the specified batch size, break the loop\n",
        "            if text.size(0) != batch_size:\n",
        "                break\n",
        "\n",
        "            # Reshape text and mask tensors\n",
        "            text = text.reshape(batch_size, -1)\n",
        "            mask = mask.reshape(batch_size, -1)\n",
        "\n",
        "            # Move tensors to GPU if available\n",
        "            if USE_CUDA:\n",
        "                text = text.cuda()\n",
        "                mask = mask.cuda()\n",
        "                y = y.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            loss, logits = net(text, mask, y)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += text.size(0)\n",
        "            correct += predicted.data.eq(y.data).cpu().sum()\n",
        "\n",
        "            # Collect predictions and true labels\n",
        "            all_pred.extend(predicted.cpu().numpy())\n",
        "            all_true.extend(y.cpu().numpy())\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({'loss': loss.item(), 'accuracy': correct.item() / total})\n",
        "\n",
        "        # After the end of each epoch, compute metrics\n",
        "        accuracy = correct.cpu().numpy().tolist()/total\n",
        "        loss = loss.detach().cpu()\n",
        "\n",
        "        # Compute F1 scores\n",
        "        macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
        "        micro_f1 = f1_score(all_true, all_pred, average='micro')\n",
        "        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n",
        "\n",
        "        epoch_losses.append(loss)\n",
        "        epoch_accuracies.append(accuracy)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Loss: {loss.mean().numpy().tolist()}, Accuracy: {accuracy}\")\n",
        "        print(f\"Macro F1: {macro_f1}, Micro F1: {micro_f1}, Weighted F1: {weighted_f1}\")\n",
        "\n",
        "        print(\"Validation...\")\n",
        "\n",
        "        dev_acc, dev_macro_f1, dev_micro_f1, dev_weighted_f1, dev_precision_list, dev_recall_list= eval(net, dev_dataset, batch_size)\n",
        "\n",
        "        print()\n",
        "\n",
        "    torch.save(model.state_dict(), 'RoBERTa_EX1_80_model.pth')\n",
        "\n",
        "    return epoch_losses, epoch_accuracies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtO_T2q5YWdl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "def eval(net, dataset, batch_size):\n",
        "    # Set the network to evaluation mode\n",
        "    net.eval()\n",
        "\n",
        "    # Create an iterator for the evaluation dataset\n",
        "    eval_iter = DataLoader(dataset, batch_size, shuffle=False)\n",
        "\n",
        "    # Lists to store predictions and true labels\n",
        "    all_pred = []\n",
        "    all_true = []\n",
        "\n",
        "    # Lists to store evaluation metrics\n",
        "    acc_list = []\n",
        "    macro_f1_list = []\n",
        "    micro_f1_list = []\n",
        "    weighted_f1_list = []\n",
        "    precision_list = []  # List to store precision scores\n",
        "    recall_list = []     # List to store recall scores\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct = 0  # Counter for correctly classified samples\n",
        "        total = 0    # Counter for total samples\n",
        "        # Progress bar for visualization during evaluation\n",
        "        progress_bar = tqdm(eval_iter, desc='Evaluation', unit='batch')\n",
        "\n",
        "        # Iterate through each batch in the evaluation dataset\n",
        "        for text, mask, y in progress_bar:\n",
        "            # If the batch size is not as expected, break the loop\n",
        "            if text.size(0) != batch_size:\n",
        "                break\n",
        "\n",
        "            # Reshape text and mask tensors\n",
        "            text = text.reshape(batch_size, -1)\n",
        "            mask = mask.reshape(batch_size, -1)\n",
        "\n",
        "            # Move tensors to GPU if available\n",
        "            if USE_CUDA:\n",
        "                text, mask, y = text.cuda(), mask.cuda(), y.cuda()\n",
        "\n",
        "            # Forward pass through the network\n",
        "            outputs = net(text, mask)\n",
        "            # Unpack outputs if it's a tuple (contains both loss and logits)\n",
        "            loss, logits = outputs if isinstance(outputs, tuple) else (None, outputs)\n",
        "\n",
        "            # Calculate predicted labels\n",
        "            _, predicted = torch.max(logits, 1)\n",
        "            # Update total and correct counts\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "            # Store predicted and true labels for computing F1 score later\n",
        "            all_pred.extend(predicted.cpu().numpy())\n",
        "            all_true.extend(y.cpu().numpy())\n",
        "\n",
        "            # Calculate accuracy for the current batch\n",
        "            accuracy = correct / total\n",
        "            progress_bar.set_postfix({'accuracy': accuracy})\n",
        "\n",
        "        # Calculate overall accuracy\n",
        "        acc = correct / total\n",
        "        # Calculate F1 scores\n",
        "        macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
        "        micro_f1 = f1_score(all_true, all_pred, average='micro')\n",
        "        weighted_f1 = f1_score(all_true, all_pred, average='weighted')\n",
        "        # Calculate precision and recall\n",
        "        precision = precision_score(all_true, all_pred, average='macro')\n",
        "        recall = recall_score(all_true, all_pred, average='macro')\n",
        "\n",
        "        # Append metrics to respective lists\n",
        "        acc_list.append(acc)\n",
        "        macro_f1_list.append(macro_f1)\n",
        "        micro_f1_list.append(micro_f1)\n",
        "        weighted_f1_list.append(weighted_f1)\n",
        "        precision_list.append(precision)\n",
        "        recall_list.append(recall)\n",
        "\n",
        "        # Print evaluation results\n",
        "        print(f\"Eval Result: right {correct}, total {total}, Acc: {acc:.4f}\")\n",
        "        print(f\"Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}, Weighted F1: {weighted_f1:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "        # Return evaluation metrics lists\n",
        "        return acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxJrdSDTv07h"
      },
      "outputs": [],
      "source": [
        "# Load data from JSON file\n",
        "train_data = load_tacred_dataset('/home/featurize/data/different_size_dataset/80/train_80%.json')\n",
        "dev_data = load_tacred_dataset('/home/featurize/data/TACRED/dev.json')\n",
        "test_data = load_tacred_dataset('/home/featurize/data/TACRED/test.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN2PJFW2v1HN",
        "outputId": "ff498652-a05a-4e2a-90b6-fa67731c674d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54496\n",
            "22631\n",
            "15509\n"
          ]
        }
      ],
      "source": [
        "# ent1+ent2+text dataset\n",
        "train_info = prepare_data(train_data)\n",
        "dev_info = prepare_data(dev_data)\n",
        "test_info = prepare_data(test_data)\n",
        "\n",
        "print(len(train_info))\n",
        "print(len(dev_info))\n",
        "print(len(test_info))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpwOfzpzOiEf",
        "outputId": "0a7861a4-8fec-4cca-f9d1-6d9546caf5e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "54496\n",
            "22631\n",
            "15509\n"
          ]
        }
      ],
      "source": [
        "train_data = process_relation_extraction_data(train_info, 128)\n",
        "dev_data = process_relation_extraction_data(dev_info, 128)\n",
        "test_data = process_relation_extraction_data(test_info, 128)\n",
        "\n",
        "print(len(train_data['label']))\n",
        "print(len(dev_data['label']))\n",
        "print(len(test_data['label']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jqApYg8YWdl",
        "outputId": "319bb072-8009-4d4e-d75d-d6a7652b119b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20945/307683292.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  text = torch.tensor(text)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--train data--\n",
            "torch.Size([54496, 1, 128])\n",
            "torch.Size([54496, 1, 128])\n",
            "torch.Size([54496])\n",
            "--eval data--\n",
            "torch.Size([22631, 1, 128])\n",
            "torch.Size([22631, 1, 128])\n",
            "torch.Size([22631])\n",
            "--test data--\n",
            "torch.Size([15509, 1, 128])\n",
            "torch.Size([15509, 1, 128])\n",
            "torch.Size([15509])\n"
          ]
        }
      ],
      "source": [
        "# Preprocess train data\n",
        "train_text, train_mask, train_label = convert_data_to_tensors(train_data)\n",
        "print(\"--train data--\")\n",
        "print(train_text.shape)\n",
        "print(train_mask.shape)\n",
        "print(train_label.shape)\n",
        "\n",
        "# Preprocess dev data\n",
        "dev_text, dev_mask, dev_label = convert_data_to_tensors(dev_data)\n",
        "print(\"--eval data--\")\n",
        "print(dev_text.shape)\n",
        "print(dev_mask.shape)\n",
        "print(dev_label.shape)\n",
        "\n",
        "# Preprocess test data\n",
        "test_text, test_mask, test_label = convert_data_to_tensors(test_data)\n",
        "print(\"--test data--\")\n",
        "print(test_text.shape)\n",
        "print(test_mask.shape)\n",
        "print(test_label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5plRuzg4U_4u"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(train_text, train_mask, train_label)\n",
        "dev_dataset = torch.utils.data.TensorDataset(dev_text, dev_mask, dev_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "l-ofVsb7V6O4",
        "outputId": "fbff24fa-2337-4f7a-b927-164210bc158c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|██████████| 3406/3406 [04:43<00:00, 12.02batch/s, loss=0.286, accuracy=0.828] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "Loss: 0.2859255373477936, Accuracy: 0.8277121256605989\n",
            "Macro F1: 0.08742568882672813, Micro F1: 0.827712125660599, Weighted F1: 0.7783901028334086\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:42<00:00, 32.98batch/s, accuracy=0.816]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 18465, total 22624, Acc: 0.8162\n",
            "Macro F1: 0.1918, Micro F1: 0.8162, Weighted F1: 0.7706\n",
            "Precision: 0.2595, Recall: 0.1813\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.42batch/s, loss=0.48, accuracy=0.874]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/15\n",
            "Loss: 0.479958176612854, Accuracy: 0.8735687022900763\n",
            "Macro F1: 0.3564654790514785, Micro F1: 0.8735687022900763, Weighted F1: 0.855931833604945\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.70batch/s, accuracy=0.841]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19033, total 22624, Acc: 0.8413\n",
            "Macro F1: 0.3368, Micro F1: 0.8413, Weighted F1: 0.8065\n",
            "Precision: 0.4983, Recall: 0.2943\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.40batch/s, loss=0.248, accuracy=0.893] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/15\n",
            "Loss: 0.24797911942005157, Accuracy: 0.8927260716382853\n",
            "Macro F1: 0.4951776167120667, Micro F1: 0.8927260716382853, Weighted F1: 0.8835423903579418\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.65batch/s, accuracy=0.848]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19193, total 22624, Acc: 0.8483\n",
            "Macro F1: 0.4662, Micro F1: 0.8483, Weighted F1: 0.8346\n",
            "Precision: 0.5734, Recall: 0.4342\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15: 100%|██████████| 3406/3406 [04:35<00:00, 12.36batch/s, loss=0.381, accuracy=0.909] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/15\n",
            "Loss: 0.38075315952301025, Accuracy: 0.9089290957134468\n",
            "Macro F1: 0.5894230674885355, Micro F1: 0.9089290957134468, Weighted F1: 0.9031944804279178\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.64batch/s, accuracy=0.86] \n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19454, total 22624, Acc: 0.8599\n",
            "Macro F1: 0.5054, Micro F1: 0.8599, Weighted F1: 0.8464\n",
            "Precision: 0.5859, Recall: 0.4792\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.41batch/s, loss=0.43, accuracy=0.923]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/15\n",
            "Loss: 0.4300401508808136, Accuracy: 0.9229484732824428\n",
            "Macro F1: 0.6424622332205538, Micro F1: 0.9229484732824428, Weighted F1: 0.9188972664454687\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.66batch/s, accuracy=0.861]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19480, total 22624, Acc: 0.8610\n",
            "Macro F1: 0.5133, Micro F1: 0.8610, Weighted F1: 0.8470\n",
            "Precision: 0.6416, Recall: 0.4615\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15: 100%|██████████| 3406/3406 [04:33<00:00, 12.45batch/s, loss=0.126, accuracy=0.937]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/15\n",
            "Loss: 0.1256888508796692, Accuracy: 0.9372064004697592\n",
            "Macro F1: 0.7096487615362621, Micro F1: 0.9372064004697592, Weighted F1: 0.9345604565355032\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.81batch/s, accuracy=0.856]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19367, total 22624, Acc: 0.8560\n",
            "Macro F1: 0.5500, Micro F1: 0.8560, Weighted F1: 0.8500\n",
            "Precision: 0.6235, Recall: 0.5362\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.41batch/s, loss=0.288, accuracy=0.949]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/15\n",
            "Loss: 0.2884180545806885, Accuracy: 0.9494825308279506\n",
            "Macro F1: 0.7714316547484983, Micro F1: 0.9494825308279506, Weighted F1: 0.9480376863815891\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.65batch/s, accuracy=0.853]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19298, total 22624, Acc: 0.8530\n",
            "Macro F1: 0.5391, Micro F1: 0.8530, Weighted F1: 0.8481\n",
            "Precision: 0.6111, Recall: 0.5180\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15: 100%|██████████| 3406/3406 [04:33<00:00, 12.47batch/s, loss=0.068, accuracy=0.961]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/15\n",
            "Loss: 0.06795675307512283, Accuracy: 0.9611164122137404\n",
            "Macro F1: 0.8101521838700899, Micro F1: 0.9611164122137404, Weighted F1: 0.960185180335678\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:42<00:00, 33.14batch/s, accuracy=0.856]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19362, total 22624, Acc: 0.8558\n",
            "Macro F1: 0.5239, Micro F1: 0.8558, Weighted F1: 0.8452\n",
            "Precision: 0.6222, Recall: 0.4796\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15: 100%|██████████| 3406/3406 [04:33<00:00, 12.45batch/s, loss=0.0169, accuracy=0.971]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/15\n",
            "Loss: 0.016938338056206703, Accuracy: 0.9709886964180857\n",
            "Macro F1: 0.8566383722038654, Micro F1: 0.9709886964180857, Weighted F1: 0.9704951708388769\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.58batch/s, accuracy=0.853]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19303, total 22624, Acc: 0.8532\n",
            "Macro F1: 0.5345, Micro F1: 0.8532, Weighted F1: 0.8467\n",
            "Precision: 0.6000, Recall: 0.5032\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.43batch/s, loss=0.158, accuracy=0.977]   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/15\n",
            "Loss: 0.15820422768592834, Accuracy: 0.9767872871403406\n",
            "Macro F1: 0.8769044516992505, Micro F1: 0.9767872871403406, Weighted F1: 0.9764256182216842\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.66batch/s, accuracy=0.838]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 18963, total 22624, Acc: 0.8382\n",
            "Macro F1: 0.5492, Micro F1: 0.8382, Weighted F1: 0.8417\n",
            "Precision: 0.5482, Recall: 0.5853\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15: 100%|██████████| 3406/3406 [04:34<00:00, 12.39batch/s, loss=0.0218, accuracy=0.982]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/15\n",
            "Loss: 0.02179173193871975, Accuracy: 0.9821454785672343\n",
            "Macro F1: 0.8997243082292761, Micro F1: 0.9821454785672343, Weighted F1: 0.9819615891056651\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.78batch/s, accuracy=0.853]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19292, total 22624, Acc: 0.8527\n",
            "Macro F1: 0.5622, Micro F1: 0.8527, Weighted F1: 0.8504\n",
            "Precision: 0.5820, Recall: 0.5685\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15: 100%|██████████| 3406/3406 [04:35<00:00, 12.37batch/s, loss=0.0658, accuracy=0.985]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12/15\n",
            "Loss: 0.0658058300614357, Accuracy: 0.9850447739283618\n",
            "Macro F1: 0.9148579560233158, Micro F1: 0.9850447739283618, Weighted F1: 0.9849165134274046\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.70batch/s, accuracy=0.853]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19287, total 22624, Acc: 0.8525\n",
            "Macro F1: 0.5448, Micro F1: 0.8525, Weighted F1: 0.8478\n",
            "Precision: 0.6359, Recall: 0.5139\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15: 100%|██████████| 3406/3406 [04:33<00:00, 12.44batch/s, loss=0.0213, accuracy=0.987]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13/15\n",
            "Loss: 0.02133133076131344, Accuracy: 0.9872834703464475\n",
            "Macro F1: 0.9230674717125289, Micro F1: 0.9872834703464475, Weighted F1: 0.9871887051046606\n",
            "Validation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluation: 100%|█████████▉| 1414/1415 [00:43<00:00, 32.72batch/s, accuracy=0.851]\n",
            "/environment/miniconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval Result: right 19243, total 22624, Acc: 0.8506\n",
            "Macro F1: 0.5567, Micro F1: 0.8506, Weighted F1: 0.8481\n",
            "Precision: 0.5767, Recall: 0.5520\n",
            "\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15: 100%|█████████▉| 3398/3406 [04:34<00:00, 12.34batch/s, loss=0.00411, accuracy=0.99]  "
          ]
        }
      ],
      "source": [
        "epoch_losses, epoch_accuracies = train(model, train_dataset, dev_dataset, 15, 0.002, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKFNJJFxYWdm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs + 1), epoch_losses, label='Loss')\n",
        "plt.title('Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs + 1), epoch_accuracies, label='Accuracy', color='orange')\n",
        "plt.title('Accuracy per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok5WkV7tYWdm"
      },
      "outputs": [],
      "source": [
        "test_dataset = torch.utils.data.TensorDataset(test_text, test_mask, test_label)\n",
        "acc_list, macro_f1_list, micro_f1_list, weighted_f1_list, precision_list, recall_list = eval(model, test_dataset, 16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03Y-i6i8YWdm"
      },
      "outputs": [],
      "source": [
        "print(acc_list)\n",
        "print(precision_list)\n",
        "print(recall_list)\n",
        "print()\n",
        "print(macro_f1_list)\n",
        "print(micro_f1_list)\n",
        "print(weighted_f1_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}