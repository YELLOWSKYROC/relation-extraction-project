{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi3IQsweYekY",
        "outputId": "571f31b6-c377-4e2e-ac02-3c8d96ac4a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmHw36rwdBOn",
        "outputId": "92dfaaba-9421-4b72-9709-7ce08b6f8dfe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "import torch.nn.functional\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertModel, AdamW\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaModel"
      ],
      "metadata": {
        "id": "vPeOwJyaYlPE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of relation types\n",
        "keys = ['no_relation', 'per:title', 'org:top_members/employees',\n",
        "        'org:country_of_headquarters', 'per:parents', 'per:age',\n",
        "        'per:countries_of_residence', 'per:children', 'org:alternate_names',\n",
        "        'per:charges', 'per:cities_of_residence', 'per:origin', 'org:founded_by',\n",
        "        'per:employee_of', 'per:siblings', 'per:alternate_names', 'org:website',\n",
        "        'per:religion', 'per:stateorprovince_of_death', 'org:parents',\n",
        "        'org:subsidiaries', 'per:other_family', 'per:stateorprovinces_of_residence',\n",
        "        'org:members', 'per:cause_of_death', 'org:member_of',\n",
        "        'org:number_of_employees/members', 'per:country_of_birth',\n",
        "        'org:shareholders', 'org:stateorprovince_of_headquarters',\n",
        "        'per:city_of_death', 'per:date_of_birth', 'per:spouse',\n",
        "        'org:city_of_headquarters', 'per:date_of_death', 'per:schools_attended',\n",
        "        'org:political/religious_affiliation', 'per:country_of_death',\n",
        "        'org:founded', 'per:stateorprovince_of_birth', 'per:city_of_birth',\n",
        "        'org:dissolved']\n",
        "\n",
        "# Assigning indices to the list elements and storing them in a dictionary\n",
        "rel2id = {key: idx for idx, key in enumerate(keys)}\n",
        "id2rel = {idx: key for key, idx in rel2id.items()}"
      ],
      "metadata": {
        "id": "xsOwUy2tYlRp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "class BERT_Classifier(nn.Module):\n",
        "    def __init__(self, label_num):\n",
        "        super().__init__()\n",
        "        # Initialize the BERT encoder from pre-trained weights\n",
        "        self.encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(768, label_num)  # 768 is the hidden size of BERT\n",
        "        # Cross-entropy loss criterion\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, attention_mask, label=None):\n",
        "        # Pass the input through the BERT encoder\n",
        "        x = self.encoder(x, attention_mask=attention_mask)[0]  # Output is tuple (last_hidden_state, pooler_output), we take the last_hidden_state\n",
        "        # Take only the first token's output (CLS token)\n",
        "        x = x[:, 0, :]\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # Pass through the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        # If label is not provided, return logits only\n",
        "        if label is None:\n",
        "            return None, x\n",
        "        else:\n",
        "            # Calculate the cross-entropy loss and return both loss and logits\n",
        "            return self.criterion(x, label), x\n",
        "\n",
        "class RoBERTa_Classifier(nn.Module):\n",
        "    def __init__(self, label_num):\n",
        "        super().__init__()\n",
        "        # Initialize the RoBERTa encoder from pre-trained weights\n",
        "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        # Dropout layer to prevent overfitting\n",
        "        self.dropout = nn.Dropout(0.1, inplace=False)\n",
        "        # Fully connected layer for classification\n",
        "        self.fc = nn.Linear(768, label_num)  # 768 is the hidden size of RoBERTa\n",
        "        # Cross-entropy loss criterion\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, x, attention_mask, label=None):\n",
        "        # Pass the input through the RoBERTa encoder\n",
        "        x = self.encoder(x, attention_mask=attention_mask)[0]  # Output is tuple (last_hidden_state, pooler_output), we take the last_hidden_state\n",
        "        # Take only the first token's output (CLS token)\n",
        "        x = x[:, 0, :]\n",
        "        # Apply dropout\n",
        "        x = self.dropout(x)\n",
        "        # Pass through the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        # If label is not provided, return logits only\n",
        "        if label is None:\n",
        "            return None, x\n",
        "        else:\n",
        "            # Calculate the cross-entropy loss and return both loss and logits\n",
        "            return self.criterion(x, label), x"
      ],
      "metadata": {
        "id": "XRjbUZi3YlUs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_relation(model_tpye, ent1, ent2, text, model_path, rel2id, id2rel):\n",
        "    # Define the device based on CUDA availability\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    labels_num = len(rel2id)\n",
        "\n",
        "    if model_tpye == \"BERT\":\n",
        "        tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "        model = BERT_Classifier(labels_num).to(device)\n",
        "    elif model_tpye == \"RoBERTa\":\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n",
        "        model = RoBERTa_Classifier(labels_num).to(device)\n",
        "    else:\n",
        "        print(\"Wrong model type!\")\n",
        "\n",
        "    # Load the model state\n",
        "    state_dict = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.eval()\n",
        "\n",
        "    # Preparing the inputs for the model\n",
        "    max_length = 128\n",
        "    sentence = ent1 + ent2 + text\n",
        "    indexed_tokens = tokenizer.encode(sentence, add_special_tokens=True, max_length=max_length, truncation=True)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
        "\n",
        "    # Creating attention mask\n",
        "    attention_mask = torch.zeros_like(tokens_tensor).long()\n",
        "    attention_mask[0, :len(indexed_tokens)] = 1\n",
        "\n",
        "    # Performing the inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor, attention_mask=attention_mask)\n",
        "        logits = outputs[0] if len(outputs) == 1 else outputs[1]\n",
        "        _, predicted = torch.max(logits, dim=1)\n",
        "        relation_id = predicted.item()\n",
        "\n",
        "    # Output the predicted relation\n",
        "    predicted_relation = id2rel[relation_id]\n",
        "\n",
        "    return predicted_relation"
      ],
      "metadata": {
        "id": "gaSfCc0JTCor"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input what you want\n",
        "text = \"He has served as a policy aide to the late U.S. Senator Alan Cranston , as National Issues Director for the 2004 presidential campaign of Congressman Dennis Kucinich , as a co-founder of Progressive Democrats of America and as a member of the international policy department at the RAND Corporation think tank before all that .\"\n",
        "ent1 = \"Progressive Democrats of America\"\n",
        "ent2 = \"international policy department\""
      ],
      "metadata": {
        "id": "nshrjy9CYlgG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo predicted Relation\n",
        "model_path = \"/content/Drive/MyDrive/model/BERT_EX1_100_model.pth\"\n",
        "predicted_relation = predict_relation(\"BERT\", ent1, ent2, text, model_path, rel2id, id2rel)\n",
        "print(f\"Predicted Relation: {predicted_relation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYqnUEaZsT9W",
        "outputId": "8afea761-227c-4b7e-8522-5f91d697db17"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Relation: no_relation\n"
          ]
        }
      ]
    }
  ]
}